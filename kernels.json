[
  {
    "kernel_name": "void deep_ep::intranode::cached_notify_combine<8>(void**, int*, int, int, int, int**, int, int)",
    "kernel_implementation": "deep_ep/buffer.py(306): combine",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/3646f6bb3e42ef31b29e4bae3244a14333a2ba9b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py#L471",
    "call_stack": "cudaLaunchKernelExC <- deep_ep/buffer.py(306): combine <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(451): _combine_core <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(444): combine_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(740): combine_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(720): combine <- sglang/srt/two_batch_overlap.py(624): _execute <- sglang/srt/two_batch_overlap.py(636): combine <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- multiprocessing/process.py(108): run <- multiprocessing/process.py(314): _bootstrap <- multiprocessing/spawn.py(129): _main <- multiprocessing/spawn.py(116): spawn_main <- <string>(1): <module>",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          7168
        ],
        "example_dtype": "torch.bfloat16",
        "description": "x"
      }
    ]
  },
  {
    "kernel_name": "void deep_ep::intranode::combine<__nv_bfloat16, 8, 768>(__nv_bfloat16*, float*, __nv_bfloat16 const*, float const*, int const*, int const*, int const*, int*, int, int, int, int, void**, int, int, int)",
    "kernel_implementation": "deep_ep/buffer.py(306): combine",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/3646f6bb3e42ef31b29e4bae3244a14333a2ba9b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py#L471",
    "call_stack": "cudaLaunchKernelExC <- deep_ep/buffer.py(306): combine <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(448): _combine_core <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(441): combine_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(737): combine_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(717): combine <- sglang/srt/two_batch_overlap.py(624): _execute <- sglang/srt/two_batch_overlap.py(636): combine <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- multiprocessing/process.py(108): run <- multiprocessing/process.py(314): _bootstrap <- multiprocessing/spawn.py(129): _main <- multiprocessing/spawn.py(116): spawn_main <- <string>(1): <module>",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          7168
        ],
        "example_dtype": "torch.bfloat16",
        "description": "x"
      }
    ]
  },
  {
    "kernel_name": "void deep_ep::intranode::notify_dispatch<8>(int const*, int*, int const*, int*, int, int, int, bool const*, int*, int*, int, int, void**, int**, int, int)",
    "kernel_implementation": "deep_ep/buffer.py(231): dispatch",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/3646f6bb3e42ef31b29e4bae3244a14333a2ba9b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py#L349",
    "call_stack": "cudaLaunchKernelExC <- deep_ep/buffer.py(231): dispatch <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(300): _dispatch_core <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(241): dispatch_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(714): dispatch_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(694): dispatch <- sglang/srt/two_batch_overlap.py(624): _execute <- sglang/srt/two_batch_overlap.py(627): dispatch <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- multiprocessing/process.py(108): run <- multiprocessing/process.py(314): _bootstrap <- multiprocessing/spawn.py(129): _main <- multiprocessing/spawn.py(116): spawn_main <- <string>(1): <module>",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          0,
          128,
          7168
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "x[0]"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          128,
          56
        ],
        "example_dtype": "torch.float32",
        "description": "x[1]"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          128,
          8
        ],
        "example_dtype": "torch.int64",
        "description": "topk_idx"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          128,
          8
        ],
        "example_dtype": "torch.float32",
        "description": "topk_weights"
      }
    ]
  },
  {
    "kernel_name": "void deep_ep::intranode::dispatch<8, 512>(int4*, float*, int*, long*, float*, int*, int*, int4 const*, float const*, long const*, float const*, bool const*, int const*, int, int, int, int, int, void**, int, int, int)",
    "kernel_implementation": "deep_ep/buffer.py(231): dispatch",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/3646f6bb3e42ef31b29e4bae3244a14333a2ba9b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py#L349",
    "call_stack": "cudaLaunchKernelExC <- deep_ep/buffer.py(231): dispatch <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(300): _dispatch_core <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(241): dispatch_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(714): dispatch_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(694): dispatch <- sglang/srt/two_batch_overlap.py(624): _execute <- sglang/srt/two_batch_overlap.py(627): dispatch <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- multiprocessing/process.py(108): run <- multiprocessing/process.py(314): _bootstrap <- multiprocessing/spawn.py(129): _main <- multiprocessing/spawn.py(116): spawn_main <- <string>(1): <module>",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          0,
          128,
          7168
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "x[0]"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          128,
          56
        ],
        "example_dtype": "torch.float32",
        "description": "x[1]"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          128,
          8
        ],
        "example_dtype": "torch.int64",
        "description": "topk_idx"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          128,
          8
        ],
        "example_dtype": "torch.float32",
        "description": "topk_weights"
      }
    ]
  },
  {
    "kernel_name": "write_req_to_token_pool_triton",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/975a5ec69c147eb68357b565aa63bcd3b56f6942/python/sglang/srt/managers/schedule_batch.py#L1838",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/srt/managers/schedule_batch.py(1117): prepare_for_extend <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- multiprocessing/process.py(108): run <- multiprocessing/process.py(314): _bootstrap <- multiprocessing/spawn.py(129): _main <- multiprocessing/spawn.py(116): spawn_main <- <string>(1): <module>",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          4096,
          163844
        ],
        "example_dtype": "torch.int32",
        "description": "req_to_token_ptr"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "torch.int64",
        "description": "req_pool_indices"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "torch.int64",
        "description": "pre_lens"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "torch.int64",
        "description": "seq_lens"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "torch.int64",
        "description": "extend_lens"
      },
      {
        "id": 5,
        "role": "input",
        "example_dim": [
          2048
        ],
        "example_dtype": "torch.int64",
        "description": "out_cache_loc"
      }
    ]
  },
  {
    "kernel_name": "compute_position_kernel",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/975a5ec69c147eb68357b565aa63bcd3b56f6942/python/sglang/srt/model_executor/forward_batch_info.py#L703",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/srt/model_executor/forward_batch_info.py(653): compute_position_triton <- sglang/srt/model_executor/forward_batch_info.py(261): init_new <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- multiprocessing/process.py(108): run <- multiprocessing/process.py(314): _bootstrap <- multiprocessing/spawn.py(129): _main <- multiprocessing/spawn.py(116): spawn_main <- <string>(1): <module>",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "torch.int32",
        "description": "positions"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "torch.int32",
        "description": "extend_start_loc"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "int",
        "description": "extend_prefix_lens"
      }
    ]
  },
  {
    "kernel_name": "create_flashinfer_kv_indices_triton",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/975a5ec69c147eb68357b565aa63bcd3b56f6942/python/sglang/srt/layers/attention/utils.py#L6",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/srt/layers/attention/flashinfer_backend.py(898): call_begin_forward <- sglang/srt/layers/attention/flashinfer_backend.py(786): update_single_wrapper <- sglang/srt/layers/attention/flashinfer_backend.py(197): init_forward_metadata <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          4096,
          163844
        ],
        "example_dtype": "torch.int32",
        "description": "req_to_token_ptr"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "torch.int64",
        "description": "req_pool_indices_ptr"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "torch.int32",
        "description": "page_kernel_lens_ptr"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          2
        ],
        "example_dtype": "torch.int32",
        "description": "kv_indptr"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [
          0
        ],
        "example_dtype": "torch.int32",
        "description": "kv_indices"
      }
    ]
  },
  {
    "kernel_name": "fused_ge_lt___and____2266955281149801844",
    "kernel_implementation": "fused_ge_lt___and____2266955281149801844",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/975a5ec69c147eb68357b565aa63bcd3b56f6942/python/sglang/srt/layers/vocab_parallel_embedding.py#L152",
    "call_stack": "cuLaunchKernel <- fused_ge_lt___and____2266955281149801844 <- get_masked_input_and_mask <- sglang/srt/layers/vocab_parallel_embedding.py(476): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: VocabParallelEmbedding_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          2048
        ],
        "example_dtype": "torch.int64",
        "description": "input_"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "int",
        "description": "org_vocab_start_index"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "int",
        "description": "org_vocab_end_index"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [],
        "example_dtype": "int",
        "description": "num_org_vocab_padding"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [],
        "example_dtype": "int",
        "description": "added_vocab_start_index"
      },
      {
        "id": 5,
        "role": "input",
        "example_dim": [],
        "example_dtype": "int",
        "description": "added_vocab_start_index"
      }
    ]
  },
  {
    "kernel_name": "triton_poi_fused_clamp_sub_0",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "clamp and sub on tensor",
    "source_code": "clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- <string>(1): launcher <- triton_poi_fused_clamp_sub_0 <- torch/_inductor/runtime/triton_heuristics.py(1018): run <- /tmp/torchinductor_root/t4/ct4r2vl6rdfho3mmvljl4obpxjy2qteq7p5tnltaqyhxxlpzue2z.py(86): call <- torch/_inductor/utils.py(2126): run <- torch/_inductor/output_code.py(463): __call__ <- torch/_functorch/_aot_autograd/runtime_wrappers.py(476): wrapper <- torch/_functorch/_aot_autograd/runtime_wrappers.py(664): inner_fn <- torch/_functorch/_aot_autograd/utils.py(116): call_func_at_runtime_with_args <- torch/_functorch/_aot_autograd/runtime_wrappers.py(286): runtime_wrapper <- torch/_functorch/aot_autograd.py(1180): forward <- torch/_dynamo/eval_frame.py(738): _fn <- sglang/srt/model_executor/forward_batch_info.py(719): clamp_position <- Torch-Compiled Region: 0/0 <- torch/_dynamo/eval_frame.py(540): _fn <- sglang/srt/model_executor/forward_batch_info.py(261): init_new <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "long int",
        "description": "input tensor"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "long int",
        "description": "min"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "max"
      }
    ]
  },
  {
    "kernel_name": "fused_rmsnorm_kernel",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/14229ccf8ff5b23272c7e07f1650534682bb0739/python/sglang/srt/layers/elementwise.py#L222",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/srt/layers/elementwise.py(266): fused_rmsnorm <- sglang/srt/models/grok.py(350): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1DecoderLayer_0 <- sglang/srt/models/grok.py(437): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Model_0 <- sglang/srt/models/grok.py(572): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          6144
        ],
        "example_dtype": "torch.bfloat16",
        "description": "activ_ptr"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1024,
          6144
        ],
        "example_dtype": "torch.bfloat16",
        "description": "weight_ptr"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          6144
        ],
        "example_dtype": "torch.bfloat16",
        "description": "hidden_dim"
      }
    ]
  },
  {
    "kernel_name": "void flashinfer::BatchQKApplyRotaryPosIdsCosSinCacheHeadParallelismKernel<false, 128u, 8u, 16u, __nv_bfloat16, long>(__nv_bfloat16*, __nv_bfloat16*, __nv_bfloat16*, __nv_bfloat16*, float*, long*, unsigned int, unsigned int, unsigned int, unsigned int, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long)",
    "kernel_implementation": "sgl_kernel::apply_rope_pos_ids_cos_sin_cache",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/14229ccf8ff5b23272c7e07f1650534682bb0739/sgl-kernel/python/sgl_kernel/elementwise.py#L256",
    "call_stack": "cudaLaunchKernel <- sgl_kernel::apply_rope_pos_ids_cos_sin_cache <- <built-in method  of PyCapsule object at 0x7f88b221e9a0> <- torch/_ops.py(722): __call__ <- sgl_kernel/elementwise.py(204): apply_rope_with_cos_sin_cache_inplace <- sglang/srt/layers/rotary_embedding.py(149): forward_cuda <- sglang/srt/custom_op.py(34): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RotaryEmbedding_0 <- sglang/srt/models/grok.py(221): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Attention_0 <- sglang/srt/models/grok.py(350): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1DecoderLayer_0 <- sglang/srt/models/grok.py(437): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Model_0 <- sglang/srt/models/grok.py(572): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          6,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "q"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1024,
          1,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "k"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          1024,
          6,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "q"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          1024,
          1,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "k"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [
          8192,
          128
        ],
        "example_dtype": "float",
        "description": "cos_sin_cache"
      },
      {
        "id": 5,
        "role": "input",
        "example_dim": [
          1024
        ],
        "example_dtype": "long int",
        "description": "positions"
      },
      {
        "id": 6,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "is_neox"
      },
      {
        "id": 7,
        "role": "N/A",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "get_cuda_stream"
      }
    ]
  },
  {
    "kernel_name": "fused_dual_residual_rmsnorm_kernel",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/14229ccf8ff5b23272c7e07f1650534682bb0739/python/sglang/srt/layers/elementwise.py#L206",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/srt/layers/elementwise.py(187): fused_dual_residual_rmsnorm <- sglang/srt/models/grok.py(350): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1DecoderLayer_0 <- sglang/srt/models/grok.py(437): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Model_0 <- sglang/srt/models/grok.py(572): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "output",
        "example_dim": [
          1024,
          6144
        ],
        "example_dtype": "torch.bfloat16",
        "description": "output"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1024,
          6144
        ],
        "example_dtype": "torch.bfloat16",
        "description": "mid"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          1024,
          6144
        ],
        "example_dtype": "torch.bfloat16",
        "description": "x"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          1024,
          6144
        ],
        "example_dtype": "torch.bfloat16",
        "description": "residual"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [
          1,
          6144
        ],
        "example_dtype": "torch.bfloat16",
        "description": "weight1"
      },
      {
        "id": 5,
        "role": "input",
        "example_dim": [
          2,
          6144
        ],
        "example_dtype": "torch.bfloat16",
        "description": "weight2"
      }
    ]
  },
  {
    "kernel_name": "fused_moe_router_kernel",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/14229ccf8ff5b23272c7e07f1650534682bb0739/python/sglang/srt/layers/moe/router.py#L14",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/srt/layers/moe/router.py(107): fused_moe_router_impl <- sglang/srt/layers/moe/router.py(276): fused_moe_router_shim <- sglang/srt/layers/moe/topk.py(298): select_experts <- sglang/srt/layers/moe/fused_moe_triton/layer.py(156): forward_cuda <- sglang/srt/custom_op.py(34): forward <- sglang/srt/layers/moe/fused_moe_triton/layer.py(120): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(641): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/grok.py(135): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1MoE_0 <- sglang/srt/models/grok.py(350): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1DecoderLayer_0 <- sglang/srt/models/grok.py(437): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Model_0 <- sglang/srt/models/grok.py(572): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          6144
        ],
        "example_dtype": "torch.bfloat16",
        "description": "input_ptr"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          8,
          6144
        ],
        "example_dtype": "torch.float32",
        "description": "moe_router_weight_ptr"
      }
    ]
  },
  {
    "kernel_name": "void moe_align_block_size_kernel<long>(long const*, int*, int*, int*, int, int, int, int, unsigned long, int*)",
    "kernel_implementation": "sgl_kernel::moe_align_block_size",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/14229ccf8ff5b23272c7e07f1650534682bb0739/sgl-kernel/python/sgl_kernel/moe.py#L6",
    "call_stack": "cudaLaunchKernel <- sgl_kernel::moe_align_block_size <- <built-in method  of PyCapsule object at 0x7f88b23298c0> <- torch/_ops.py(722): __call__ <- sgl_kernel/moe.py(4): moe_align_block_size <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(653): moe_align_block_size <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1308): fused_experts_impl <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1079): inplace_fused_experts <- sglang::inplace_fused_experts <- <built-in method inplace_fused_experts of PyCapsule object at 0x7f88b221fcf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1236): fused_experts <- sglang/srt/layers/moe/fused_moe_triton/layer.py(156): forward_cuda <- sglang/srt/custom_op.py(34): forward <- sglang/srt/layers/moe/fused_moe_triton/layer.py(120): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(641): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/grok.py(135): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1MoE_0 <- sglang/srt/models/grok.py(350): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1DecoderLayer_0 <- sglang/srt/models/grok.py(437): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Model_0 <- sglang/srt/models/grok.py(572): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          2
        ],
        "example_dtype": "long int",
        "description": "topk_ids"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "num_experts"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "block_size"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          3064
        ],
        "example_dtype": "int",
        "description": "sorted_token_ids"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [
          24
        ],
        "example_dtype": "int",
        "description": "experts_ids"
      },
      {
        "id": 5,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "int",
        "description": "num_tokens_post_pad"
      },
      {
        "id": 6,
        "role": "input",
        "example_dim": [
          72
        ],
        "example_dtype": "int",
        "description": "token_cnts_buffer"
      },
      {
        "id": 7,
        "role": "input",
        "example_dim": [
          9
        ],
        "example_dtype": "int",
        "description": "cumsum_buffer"
      }
    ]
  },
  {
    "kernel_name": "void count_and_sort_expert_tokens_kernel<long>(long const*, int*, int*, unsigned long)",
    "kernel_implementation": "sgl_kernel::moe_align_block_size",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "",
    "call_stack": "cudaLaunchKernel <- sgl_kernel::moe_align_block_size <- <built-in method  of PyCapsule object at 0x7f88b23298c0> <- torch/_ops.py(722): __call__ <- sgl_kernel/moe.py(4): moe_align_block_size <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(653): moe_align_block_size <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1308): fused_experts_impl <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1079): inplace_fused_experts <- sglang::inplace_fused_experts <- <built-in method inplace_fused_experts of PyCapsule object at 0x7f88b221fcf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1236): fused_experts <- sglang/srt/layers/moe/fused_moe_triton/layer.py(156): forward_cuda <- sglang/srt/custom_op.py(34): forward <- sglang/srt/layers/moe/fused_moe_triton/layer.py(120): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(641): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/grok.py(135): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1MoE_0 <- sglang/srt/models/grok.py(350): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1DecoderLayer_0 <- sglang/srt/models/grok.py(437): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Model_0 <- sglang/srt/models/grok.py(572): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          2
        ],
        "example_dtype": "long int",
        "description": "topk_ids"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "num_experts"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "block_size"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          3064
        ],
        "example_dtype": "int",
        "description": "sorted_token_ids"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [
          24
        ],
        "example_dtype": "int",
        "description": "experts_ids"
      },
      {
        "id": 5,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "int",
        "description": "num_tokens_post_pad"
      },
      {
        "id": 6,
        "role": "input",
        "example_dim": [
          72
        ],
        "example_dtype": "int",
        "description": "token_cnts_buffer"
      },
      {
        "id": 7,
        "role": "input",
        "example_dim": [
          9
        ],
        "example_dtype": "int",
        "description": "cumsum_buffer"
      }
    ]
  },
  {
    "kernel_name": "fused_moe_kernel",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/14229ccf8ff5b23272c7e07f1650534682bb0739/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py#L1163",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(735): invoke_fused_moe_kernel <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1308): fused_experts_impl <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1079): inplace_fused_experts <- sglang::inplace_fused_experts <- <built-in method inplace_fused_experts of PyCapsule object at 0x7f88b221fcf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1236): fused_experts <- sglang/srt/layers/moe/fused_moe_triton/layer.py(156): forward_cuda <- sglang/srt/custom_op.py(34): forward <- sglang/srt/layers/moe/fused_moe_triton/layer.py(120): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(641): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/grok.py(135): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1MoE_0 <- sglang/srt/models/grok.py(350): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1DecoderLayer_0 <- sglang/srt/models/grok.py(437): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Model_0 <- sglang/srt/models/grok.py(572): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          6144
        ],
        "example_dtype": "c10::BFloat16",
        "description": "hidden_states"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          8,
          8192,
          6144
        ],
        "example_dtype": "c10::BFloat16",
        "description": "w1"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          8,
          6144,
          4096
        ],
        "example_dtype": "c10::BFloat16",
        "description": "w2"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          1024,
          2
        ],
        "example_dtype": "float",
        "description": "topk_weights"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [
          1024,
          2
        ],
        "example_dtype": "int",
        "description": "topk_ids"
      },
      {
        "id": 5,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "activation"
      },
      {
        "id": 6,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "apply_router_weight_on_input"
      },
      {
        "id": 7,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "use_fp8_w8a8"
      },
      {
        "id": 8,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "use_int8_w8a8"
      },
      {
        "id": 9,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "use_int8_w8a16"
      },
      {
        "id": 10,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "use_int4_w4a16"
      },
      {
        "id": 11,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "per_channel_quant"
      },
      {
        "id": 12,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "w1_scale"
      },
      {
        "id": 13,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "w2_scale"
      },
      {
        "id": 14,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "w1_zp"
      },
      {
        "id": 15,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "w2_zp"
      },
      {
        "id": 16,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "a1_scale"
      },
      {
        "id": 17,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "a2_scale"
      },
      {
        "id": 18,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "block_shape"
      }
    ]
  },
  {
    "kernel_name": "void flashinfer::activation::act_and_mul_kernel<__nv_bfloat16, &(gelu(float const&))>(__nv_bfloat16*, __nv_bfloat16 const*, int)",
    "kernel_implementation": "sgl_kernel::gelu_and_mul",
    "op_mapping": "TBD",
    "operation": "gelu and mul.",
    "source_code": "https://github.com/sgl-project/sglang/blob/3646f6bb3e42ef31b29e4bae3244a14333a2ba9b/sgl-kernel/python/sgl_kernel/elementwise.py#L212",
    "call_stack": "cudaLaunchKernel <- sgl_kernel::gelu_and_mul <- <built-in method  of PyCapsule object at 0x7f88b221eb50> <- torch/_ops.py(722): __call__ <- sgl_kernel/elementwise.py(189): gelu_and_mul <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1308): fused_experts_impl <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1079): inplace_fused_experts <- sglang::inplace_fused_experts <- <built-in method inplace_fused_experts of PyCapsule object at 0x7f88b221fcf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1236): fused_experts <- sglang/srt/layers/moe/fused_moe_triton/layer.py(156): forward_cuda <- sglang/srt/custom_op.py(34): forward <- sglang/srt/layers/moe/fused_moe_triton/layer.py(120): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(641): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/grok.py(135): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1MoE_0 <- sglang/srt/models/grok.py(350): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1DecoderLayer_0 <- sglang/srt/models/grok.py(437): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Model_0 <- sglang/srt/models/grok.py(572): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "output",
        "example_dim": [
          2048,
          4096
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          2048,
          8192
        ],
        "example_dtype": "c10::BFloat16",
        "description": "input"
      },
      {
        "id": 2,
        "role": "N/A",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "get_cuda_stream"
      }
    ]
  },
  {
    "kernel_name": "void flashinfer::BatchPrefillWithPagedKVCacheKernel<flashinfer::KernelTraits<(flashinfer::MaskMode)0, 16u, 1u, 2u, 8u, 8u, 1u, 4u, (flashinfer::PosEncodingMode)0, __nv_bfloat16, __nv_bfloat16, __nv_bfloat16, float, int, flashinfer::DefaultAttention<false, false, false, false> >, PagedParams>(PagedParams)",
    "kernel_implementation": "batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False::paged_run",
    "op_mapping": "TBD",
    "operation": "o = softmax(q@k^T)@v",
    "source_code": "https://github.com/flashinfer-ai/flashinfer/blob/3b01face12e6520abdb3ac6ac8a5797ec4521c49/flashinfer/prefill.py#L336",
    "call_stack": "cudaLaunchKernel <- batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False::paged_run <- <built-in method  of PyCapsule object at 0x7f88b221cb70> <- torch/_ops.py(722): __call__ <- flashinfer/prefill.py(308): paged_run <- flashinfer/decode.py(982): run <- flashinfer/decode.py(929): forward <- sglang/srt/layers/attention/flashinfer_backend.py(517): forward_decode <- sglang/srt/layers/attention/base_attn_backend.py(57): forward <- sglang/srt/layers/radix_attention.py(83): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RadixAttention_0 <- sglang/srt/models/grok.py(221): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Attention_0 <- sglang/srt/models/grok.py(350): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1DecoderLayer_0 <- sglang/srt/models/grok.py(437): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Model_0 <- sglang/srt/models/grok.py(572): forward <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          402653184
        ],
        "example_dtype": "unsigned char",
        "description": "float_workspace_buffer"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          8388608
        ],
        "example_dtype": "unsigned char",
        "description": "int_workspace_buffer"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          15
        ],
        "example_dtype": "long int",
        "description": "plan_info_vec"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          1,
          6,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "q"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [
          251130838,
          1,
          1,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "paged_k_cache"
      },
      {
        "id": 5,
        "role": "input",
        "example_dim": [
          251130838,
          1,
          1,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "paged_v_cache"
      },
      {
        "id": 6,
        "role": "input",
        "example_dim": [
          2
        ],
        "example_dtype": "int",
        "description": "qo_indptr"
      },
      {
        "id": 7,
        "role": "input",
        "example_dim": [
          2
        ],
        "example_dtype": "int",
        "description": "paged_kv_indptr"
      },
      {
        "id": 8,
        "role": "input",
        "example_dim": [
          1025
        ],
        "example_dtype": "int",
        "description": "paged_kv_indices"
      },
      {
        "id": 9,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "int",
        "description": "paged_kv_last_page_len"
      },
      {
        "id": 10,
        "role": "output",
        "example_dim": [
          1,
          6,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "o"
      },
      {
        "id": 11,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "maybe_lse"
      },
      {
        "id": 12,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "mask_mode"
      },
      {
        "id": 13,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "layout"
      },
      {
        "id": 14,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "window_left"
      },
      {
        "id": 15,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "enable_pdl"
      },
      {
        "id": 16,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "maybe_custom_mask"
      },
      {
        "id": 17,
        "role": "input",
        "example_dim": [
          6
        ],
        "example_dtype": "float",
        "description": "logits_soft_cap"
      },
      {
        "id": 18,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "sm_scale"
      },
      {
        "id": 19,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "rope_scale"
      },
      {
        "id": 20,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "rope_theta"
      },
      {
        "id": 21,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "token_pos_in_items_len"
      }
    ]
  },
  {
    "kernel_name": "void moe_align_block_size_small_batch_expert_kernel<long>(long const*, int*, int*, int*, int, int, unsigned long)",
    "kernel_implementation": "sgl_kernel::moe_align_block_size",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/3646f6bb3e42ef31b29e4bae3244a14333a2ba9b/sgl-kernel/python/sgl_kernel/moe.py#L6",
    "call_stack": "cudaLaunchKernel <- sgl_kernel::moe_align_block_size <- <built-in method  of PyCapsule object at 0x7f88b23298c0> <- torch/_ops.py(722): __call__ <- sgl_kernel/moe.py(4): moe_align_block_size <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(653): moe_align_block_size <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1308): fused_experts_impl <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1079): inplace_fused_experts <- sglang::inplace_fused_experts <- <built-in method inplace_fused_experts of PyCapsule object at 0x7f88b221fcf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1236): fused_experts <- sglang/srt/layers/moe/fused_moe_triton/layer.py(156): forward_cuda <- sglang/srt/custom_op.py(34): forward <- sglang/srt/layers/moe/fused_moe_triton/layer.py(120): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(641): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/grok.py(135): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1MoE_0 <- sglang/srt/models/grok.py(350): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1DecoderLayer_0 <- sglang/srt/models/grok.py(437): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Model_0 <- sglang/srt/models/grok.py(572): forward <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2
        ],
        "example_dtype": "long int",
        "description": "topk_ids"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "num_experts"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "block_size"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          122
        ],
        "example_dtype": "int",
        "description": "sorted_token_ids"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [
          8
        ],
        "example_dtype": "int",
        "description": "experts_ids"
      },
      {
        "id": 5,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "int",
        "description": "num_tokens_post_pad"
      },
      {
        "id": 6,
        "role": "input",
        "example_dim": [
          72
        ],
        "example_dtype": "int",
        "description": "token_cnts_buffer"
      },
      {
        "id": 7,
        "role": "input",
        "example_dim": [
          9
        ],
        "example_dtype": "int",
        "description": "cumsum_buffer"
      }
    ]
  },
  {
    "kernel_name": "void flashinfer::norm::RMSNormKernel<8u, __nv_bfloat16>(__nv_bfloat16*, __nv_bfloat16*, __nv_bfloat16*, unsigned int, unsigned int, unsigned int, float, float)",
    "kernel_implementation": "sgl_kernel::rmsnorm",
    "op_mapping": "TBD",
    "operation": "out[i] = (input[i] / RMS(input)) * weight[i]",
    "source_code": "https://github.com/sgl-project/sglang/blob/3646f6bb3e42ef31b29e4bae3244a14333a2ba9b/sgl-kernel/python/sgl_kernel/elementwise.py#L44",
    "call_stack": "cudaLaunchKernelExC <- sgl_kernel::rmsnorm <- <built-in method  of PyCapsule object at 0x743bd049edf0> <- torch/_ops.py(722): __call__ <- sgl_kernel/elementwise.py(9): rmsnorm <- sglang/srt/layers/layernorm.py(62): forward_cuda <- sglang/srt/layers/layernorm.py(52): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RMSNorm_0 <- sglang/srt/layers/communicator.py(176): prepare_attn <- sglang/srt/models/qwen3_moe.py(559): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeDecoderLayer_0 <- sglang/srt/models/qwen2_moe.py(431): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeModel_0 <- sglang/srt/models/qwen3_moe.py(690): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "output",
        "example_dim": [
          1024,
          4096
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1024,
          4096
        ],
        "example_dtype": "c10::BFloat16",
        "description": "input"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          4096
        ],
        "example_dtype": "c10::BFloat16",
        "description": "weight"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "epsilon"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "enable_pdl"
      }
    ]
  },
  {
    "kernel_name": "void flashinfer::norm::FusedAddRMSNormKernel<8u, __nv_bfloat16>(__nv_bfloat16*, __nv_bfloat16*, __nv_bfloat16*, unsigned int, unsigned int, unsigned int, float, float)",
    "kernel_implementation": "sgl_kernel::fused_add_rmsnorm",
    "op_mapping": "TBD",
    "operation": "Step 1: residual[i] += input[i]; Step 2: out[i] = (residual[i] / RMS(residual)) * weight[i]",
    "source_code": "https://github.com/sgl-project/sglang/blob/3646f6bb3e42ef31b29e4bae3244a14333a2ba9b/sgl-kernel/python/sgl_kernel/elementwise.py#L80",
    "call_stack": "cudaLaunchKernelExC <- sgl_kernel::fused_add_rmsnorm <- <built-in method  of PyCapsule object at 0x743bd049c120> <- torch/_ops.py(722): __call__ <- sgl_kernel/elementwise.py(45): fused_add_rmsnorm <- sglang/srt/layers/layernorm.py(62): forward_cuda <- sglang/srt/layers/layernorm.py(52): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RMSNorm_3 <- sglang/srt/layers/communicator.py(362): _gather_hidden_states <- sglang/srt/layers/communicator.py(199): prepare_mlp <- sglang/srt/models/qwen3_moe.py(559): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeDecoderLayer_0 <- sglang/srt/models/qwen2_moe.py(431): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeModel_0 <- sglang/srt/models/qwen3_moe.py(690): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          4096
        ],
        "example_dtype": "c10::BFloat16",
        "description": "input"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1024,
          4096
        ],
        "example_dtype": "c10::BFloat16",
        "description": "residual"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          4096
        ],
        "example_dtype": "c10::BFloat16",
        "description": "weight"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "epsilon"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "enable_pdl"
      }
    ]
  },
  {
    "kernel_name": "void topkGatingSoftmax<4, 128, 4, 16>(float const*, bool const*, float*, int, int*, int*, int, int, int)",
    "kernel_implementation": "sgl_kernel::topk_softmax",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/3646f6bb3e42ef31b29e4bae3244a14333a2ba9b/sgl-kernel/python/sgl_kernel/moe.py#L36",
    "call_stack": "cudaLaunchKernel <- sgl_kernel::topk_softmax <- <built-in method  of PyCapsule object at 0x743bd049c570> <- torch/_ops.py(722): __call__ <- sgl_kernel/moe.py(26): topk_softmax <- sglang/srt/layers/moe/topk.py(64): fused_topk <- sglang/srt/layers/moe/topk.py(298): select_experts <- sglang/srt/layers/moe/fused_moe_triton/layer.py(156): forward_cuda <- sglang/srt/custom_op.py(34): forward <- sglang/srt/layers/moe/fused_moe_triton/layer.py(120): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(641): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/qwen3_moe.py(168): forward_normal <- sglang/srt/models/qwen3_moe.py(152): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeSparseMoeBlock_0 <- sglang/srt/models/qwen3_moe.py(559): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeDecoderLayer_0 <- sglang/srt/models/qwen2_moe.py(431): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeModel_0 <- sglang/srt/models/qwen3_moe.py(690): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "input",
        "example_dim": [
          1024,
          8
        ],
        "example_dtype": "float",
        "description": "topk_weights"
      },
      {
        "role": "input",
        "example_dim": [
          1024,
          8
        ],
        "example_dtype": "int",
        "description": "topk_ids"
      },
      {
        "role": "output",
        "example_dim": [
          1024,
          8
        ],
        "example_dtype": "int",
        "description": "gating_output"
      },
      {
        "role": "input",
        "example_dim": [
          1024,
          128
        ],
        "example_dtype": "float",
        "description": "renormalize"
      }
    ]
  },
  {
    "kernel_name": "void flashinfer::activation::act_and_mul_kernel<__nv_bfloat16, &(silu(float const&))>(__nv_bfloat16*, __nv_bfloat16 const*, int)",
    "kernel_implementation": "sgl_kernel::silu_and_mul",
    "op_mapping": "TBD",
    "operation": "silu and mul",
    "source_code": "https://github.com/sgl-project/sglang/blob/3646f6bb3e42ef31b29e4bae3244a14333a2ba9b/sgl-kernel/python/sgl_kernel/elementwise.py#L182",
    "call_stack": "cudaLaunchKernel <- sgl_kernel::silu_and_mul <- <built-in method  of PyCapsule object at 0x743bd04abe70> <- torch/_ops.py(722): __call__ <- sgl_kernel/elementwise.py(159): silu_and_mul <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1308): fused_experts_impl <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1079): inplace_fused_experts <- sglang::inplace_fused_experts <- <built-in method inplace_fused_experts of PyCapsule object at 0x743bd049c600> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1236): fused_experts <- sglang/srt/layers/moe/fused_moe_triton/layer.py(156): forward_cuda <- sglang/srt/custom_op.py(34): forward <- sglang/srt/layers/moe/fused_moe_triton/layer.py(120): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(641): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/qwen3_moe.py(168): forward_normal <- sglang/srt/models/qwen3_moe.py(152): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeSparseMoeBlock_0 <- sglang/srt/models/qwen3_moe.py(559): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeDecoderLayer_0 <- sglang/srt/models/qwen2_moe.py(431): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeModel_0 <- sglang/srt/models/qwen3_moe.py(690): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "output",
        "example_dim": [
          8192,
          192
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      },
      {
        "role": "input",
        "example_dim": [
          8192,
          384
        ],
        "example_dtype": "c10::BFloat16",
        "description": "input"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "get_cuda_stream"
      }
    ]
  },
  {
    "kernel_name": "_per_token_group_quant_fp8",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/3646f6bb3e42ef31b29e4bae3244a14333a2ba9b/python/sglang/srt/layers/quantization/fp8_utils.py#L299",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/srt/layers/quantization/fp8_utils.py(284): triton_w8a8_block_fp8_linear <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(285): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: ReplicatedLinear_0 <- sglang/srt/models/deepseek_v2.py(855): forward_normal_prepare <- sglang/srt/models/deepseek_v2.py(802): forward_prepare <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "input",
        "example_dim": [
          2,
          1024,
          7168
        ],
        "example_dtype": "torch.bfloat16",
        "description": "input_2d"
      }
    ]
  },
  {
    "kernel_name": "void per_token_group_quant_8bit_kernel<__nv_bfloat16, c10::Float8_e4m3fn, true>(__nv_bfloat16 const*, void*, float*, int, int, int, float, float, float, int, int)",
    "kernel_implementation": "sgl_kernel::sgl_per_token_group_quant_fp8",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/3646f6bb3e42ef31b29e4bae3244a14333a2ba9b/python/sglang/srt/layers/quantization/fp8_kernel.py#L322",
    "call_stack": "cudaLaunchKernel <- sgl_kernel::sgl_per_token_group_quant_fp8 <- <built-in method  of PyCapsule object at 0x783ad4f31bf0> <- torch/_ops.py(722): __call__ <- sgl_kernel/gemm.py(85): sgl_per_token_group_quant_fp8 <- sglang/srt/layers/quantization/fp8_kernel.py(277): sglang_per_token_group_quant_fp8 <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: ColumnParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(855): forward_normal_prepare <- sglang/srt/models/deepseek_v2.py(802): forward_prepare <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "input",
        "example_dim": [
          1024,
          1536
        ],
        "example_dtype": "c10::BFloat16",
        "description": "x"
      },
      {
        "role": "output",
        "example_dim": [
          1024,
          1536
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "x_q"
      },
      {
        "role": "output",
        "example_dim": [
          1024,
          12
        ],
        "example_dtype": "float",
        "description": "x_s"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "group_size"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "epsilon"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "fp8_min"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "fp8_max"
      }
    ]
  },
  {
    "kernel_name": "void flashinfer::BatchQKApplyRotaryPosIdsCosSinCacheHeadParallelismKernel<true, 64u, 8u, 8u, __nv_bfloat16, long>(__nv_bfloat16*, __nv_bfloat16*, __nv_bfloat16*, __nv_bfloat16*, float*, long*, unsigned int, unsigned int, unsigned int, unsigned int, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long)",
    "kernel_implementation": "sgl_kernel::apply_rope_pos_ids_cos_sin_cache",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/14229ccf8ff5b23272c7e07f1650534682bb0739/sgl-kernel/python/sgl_kernel/elementwise.py#L256",
    "call_stack": "cudaLaunchKernel <- sgl_kernel::apply_rope_pos_ids_cos_sin_cache <- <built-in method  of PyCapsule object at 0x783ad4f31d10> <- torch/_ops.py(722): __call__ <- sgl_kernel/elementwise.py(204): apply_rope_with_cos_sin_cache_inplace <- sglang/srt/layers/rotary_embedding.py(149): forward_cuda <- sglang/srt/layers/rotary_embedding.py(656): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekScalingRotaryEmbedding_0 <- sglang/srt/models/deepseek_v2.py(855): forward_normal_prepare <- sglang/srt/models/deepseek_v2.py(802): forward_prepare <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          6,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "q"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1024,
          1,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "k"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          1024,
          6,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "q"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          1024,
          1,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "k"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [
          8192,
          128
        ],
        "example_dtype": "float",
        "description": "cos_sin_cache"
      },
      {
        "id": 5,
        "role": "input",
        "example_dim": [
          1024
        ],
        "example_dtype": "long int",
        "description": "positions"
      },
      {
        "id": 6,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "is_neox"
      },
      {
        "id": 7,
        "role": "N/A",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "get_cuda_stream"
      }
    ]
  },
  {
    "kernel_name": "void moe_fused_gate_kernel<cutlass::bfloat16_t, 32, 256, 8, 4, 24, 6>(void*, void*, float*, int*, long, long, long, long, double)",
    "kernel_implementation": "sgl_kernel::moe_fused_gate",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/3646f6bb3e42ef31b29e4bae3244a14333a2ba9b/sgl-kernel/python/sgl_kernel/moe.py#L41",
    "call_stack": "cudaLaunchKernel <- sgl_kernel::moe_fused_gate <- <built-in method  of PyCapsule object at 0x783ad4f33cc0> <- torch/_ops.py(722): __call__ <- sgl_kernel/moe.py(37): moe_fused_gate <- sglang/srt/layers/moe/topk.py(233): biased_grouped_topk <- sglang/srt/layers/moe/topk.py(298): select_experts <- sglang/srt/layers/quantization/fp8.py(930): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(641): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/deepseek_v2.py(333): forward_normal <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "input",
        "example_dim": [
          1024,
          256
        ],
        "example_dtype": "c10::BFloat16",
        "description": "input tensor"
      },
      {
        "role": "input",
        "example_dim": [
          256
        ],
        "example_dtype": "c10::BFloat16",
        "description": "bias"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "num_expert_group"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "topk_group"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "topk"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "num_fused_shared_experts"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "routed_scaling_factor"
      }
    ]
  },
  {
    "kernel_name": "void per_token_group_quant_8bit_kernel<__nv_bfloat16, c10::Float8_e4m3fn, false>(__nv_bfloat16 const*, void*, float*, int, int, int, float, float, float, int, int)",
    "kernel_implementation": "sgl_kernel::sgl_per_token_group_quant_fp8",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/3646f6bb3e42ef31b29e4bae3244a14333a2ba9b/python/sglang/srt/layers/quantization/fp8_kernel.py#L322",
    "call_stack": "cudaLaunchKernel <- sgl_kernel::sgl_per_token_group_quant_fp8 <- <built-in method  of PyCapsule object at 0x783ad4f31bf0> <- torch/_ops.py(722): __call__ <- sgl_kernel/gemm.py(85): sgl_per_token_group_quant_fp8 <- sglang/srt/layers/quantization/fp8_kernel.py(277): sglang_per_token_group_quant_fp8 <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(735): invoke_fused_moe_kernel <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1308): fused_experts_impl <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1079): inplace_fused_experts <- sglang::inplace_fused_experts <- <built-in method inplace_fused_experts of PyCapsule object at 0x783ad4f30330> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1236): fused_experts <- sglang/srt/layers/quantization/fp8.py(930): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(641): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/deepseek_v2.py(333): forward_normal <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "input",
        "example_dim": [
          1024,
          1536
        ],
        "example_dtype": "c10::BFloat16",
        "description": "x"
      },
      {
        "role": "output",
        "example_dim": [
          1024,
          1536
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "x_q"
      },
      {
        "role": "output",
        "example_dim": [
          1024,
          12
        ],
        "example_dtype": "float",
        "description": "x_s"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "group_size"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "epsilon"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "fp8_min"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "fp8_max"
      }
    ]
  },
  {
    "kernel_name": "set_mla_kv_buffer_kernel",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/srt/mem_cache/memory_pool.py(484): set_mla_kv_buffer_triton <- sglang/srt/mem_cache/memory_pool.py(612): set_mla_kv_buffer <- sglang/srt/layers/attention/flashinfer_mla_backend.py(435): forward_decode <- sglang/srt/layers/attention/base_attn_backend.py(57): forward <- sglang/srt/layers/radix_attention.py(83): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RadixAttention_2 <- sglang/srt/models/deepseek_v2.py(982): forward_absorb_core <- sglang/srt/models/deepseek_v2.py(837): forward_core <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": "N/A",
        "example_dtype": "N/A",
        "description": ""
      }
    ]
  },
  {
    "kernel_name": "mock_reduce_scatter",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "reduce scatter",
    "source_code": "",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/mock_dist.py(118): reduce_scatter <- torch/distributed/distributed_c10d.py(4118): reduce_scatter <- torch/distributed/c10d_logger.py(78): wrapper <- sglang/srt/distributed/parallel_state.py(463): reduce_scatter <- sglang/srt/layers/dp_attention.py(295): attn_tp_reduce_scatter <- sglang/srt/layers/communicator.py(386): _scatter_hidden_states_and_residual <- sglang/srt/layers/communicator.py(199): prepare_mlp <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": "N/A",
        "example_dtype": "N/A",
        "description": ""
      }
    ]
  },
  {
    "kernel_name": "triton_poi_fused_index_put_lift_fresh_0",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/3646f6bb3e42ef31b29e4bae3244a14333a2ba9b/python/sglang/srt/layers/moe/topk.py#L120",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- <string>(1): launcher <- triton_poi_fused_index_put_lift_fresh_0 <- torch/_inductor/runtime/triton_heuristics.py(1018): run <- /tmp/torchinductor_root/5p/c5pvn7wc3r6zwfrwehh7werxbl5gn3vduddir4rfwdmx74qjodzr.py(88): call <- torch/_inductor/utils.py(2126): run <- torch/_inductor/output_code.py(463): __call__ <- torch/_functorch/_aot_autograd/runtime_wrappers.py(476): wrapper <- torch/_functorch/_aot_autograd/runtime_wrappers.py(664): inner_fn <- torch/_functorch/_aot_autograd/utils.py(116): call_func_at_runtime_with_args <- torch/_functorch/_aot_autograd/runtime_wrappers.py(286): runtime_wrapper <- torch/_functorch/aot_autograd.py(1180): forward <- torch/_dynamo/eval_frame.py(738): _fn <- sglang/srt/layers/moe/topk.py(223): _mask_topk_ids_padded_region <- Torch-Compiled Region: 0/0 <- torch/_dynamo/eval_frame.py(540): _fn <- sglang/srt/layers/moe/topk.py(233): biased_grouped_topk <- sglang/srt/layers/moe/topk.py(298): select_experts <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "int",
        "description": ""
      },
      {
        "role": "input",
        "example_dim": [
          128,
          8
        ],
        "example_dtype": "long int",
        "description": "topk_ids"
      },
      {
        "role": "input",
        "example_dim": [
          128,
          8
        ],
        "example_dtype": "long int",
        "description": "topk_weights"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "topk"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "num_tokens_non_padded"
      }
    ]
  },
  {
    "kernel_name": "void deep_ep::internode::get_dispatch_layout<256, 32, 8>(long const*, int*, int*, int*, bool*, int, int, int, int)",
    "kernel_implementation": "deep_ep/buffer.py(202): get_dispatch_layout",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "",
    "call_stack": "cudaLaunchKernelExC <- deep_ep/buffer.py(202): get_dispatch_layout <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(300): _dispatch_core <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(241): dispatch_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(711): dispatch_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(691): dispatch <- sglang/srt/two_batch_overlap.py(624): _execute <- sglang/srt/two_batch_overlap.py(627): dispatch <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "input",
        "example_dim": "N/A",
        "example_dtype": "N/A",
        "description": ""
      }
    ]
  },
  {
    "kernel_name": "mock_notify_dispatch",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/mock_dist.py(422): fake_dispatch <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(300): _dispatch_core <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(241): dispatch_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(711): dispatch_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(691): dispatch <- sglang/srt/two_batch_overlap.py(624): _execute <- sglang/srt/two_batch_overlap.py(627): dispatch <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": []
  },
  {
    "kernel_name": "mock_dispatch",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/mock_dist.py(422): fake_dispatch <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(300): _dispatch_core <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(241): dispatch_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(711): dispatch_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(691): dispatch <- sglang/srt/two_batch_overlap.py(624): _execute <- sglang/srt/two_batch_overlap.py(627): dispatch <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": []
  },
  {
    "kernel_name": "_tma_align_input_scale_kernel",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/srt/layers/moe/ep_moe/kernels.py(1065): tma_align_input_scale <- sglang/srt/layers/moe/ep_moe/layer.py(1075): forward_deepgemm_contiguous <- sglang/srt/layers/moe/ep_moe/layer.py(931): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepEPMoE_0 <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "input",
        "example_dim": "N/A",
        "example_dtype": "N/A",
        "description": ""
      }
    ]
  },
  {
    "kernel_name": "_fwd_kernel_ep_gather",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/srt/layers/moe/ep_moe/kernels.py(975): ep_gather <- sglang/srt/layers/moe/ep_moe/layer.py(1075): forward_deepgemm_contiguous <- sglang/srt/layers/moe/ep_moe/layer.py(931): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepEPMoE_0 <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "input",
        "example_dim": [
          1024,
          7168
        ],
        "example_dtype": "torch.bfloat16",
        "description": "input"
      }
    ]
  },
  {
    "kernel_name": "mock_notify_combine",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/mock_dist.py(516): fake_combine <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(448): _combine_core <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(441): combine_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(737): combine_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(717): combine <- sglang/srt/two_batch_overlap.py(624): _execute <- sglang/srt/two_batch_overlap.py(636): combine <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": []
  },
  {
    "kernel_name": "mock_combine",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/mock_dist.py(516): fake_combine <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(448): _combine_core <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(441): combine_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(737): combine_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(717): combine <- sglang/srt/two_batch_overlap.py(624): _execute <- sglang/srt/two_batch_overlap.py(636): combine <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": []
  },
  {
    "kernel_name": "memcpy_triton_kernel",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "memcpy",
    "source_code": "sglang/srt/layers/dp_attention.py(213): memcpy_triton",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/srt/layers/dp_attention.py(213): memcpy_triton <- sglang/srt/layers/dp_attention.py(224): _dp_gather <- sglang/srt/layers/dp_attention.py(258): dp_gather_partial <- sglang/srt/layers/communicator.py(362): _gather_hidden_states <- sglang/srt/layers/communicator.py(199): prepare_mlp <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": "N/A",
        "example_dtype": "N/A",
        "description": "Input and output buffer for memory copy"
      }
    ]
  },
  {
    "kernel_name": "_fwd_kernel_ep_scatter_1",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/975a5ec69c147eb68357b565aa63bcd3b56f6942/python/sglang/srt/layers/moe/ep_moe/kernels.py#L856",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/srt/layers/moe/ep_moe/kernels.py(844): ep_scatter <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/layers/moe/ep_moe/layer.py(1075): forward_deepgemm_contiguous <- sglang/srt/layers/moe/ep_moe/layer.py(931): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepEPMoE_0 <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          7168
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "recv_x"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          8192,
          7168
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "output_tensor"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          8192,
          56
        ],
        "example_dtype": "torch.float32",
        "description": "output_tensor_scale"
      }
    ]
  },
  {
    "kernel_name": "_fwd_kernel_ep_scatter_2",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "",
    "source_code": "https://github.com/sgl-project/sglang/blob/975a5ec69c147eb68357b565aa63bcd3b56f6942/python/sglang/srt/layers/moe/ep_moe/kernels.py#L856",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/srt/layers/moe/ep_moe/kernels.py(844): ep_scatter <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/layers/moe/ep_moe/layer.py(1075): forward_deepgemm_contiguous <- sglang/srt/layers/moe/ep_moe/layer.py(931): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepEPMoE_0 <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          7168
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "recv_x"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          8192,
          7168
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "output_tensor"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          8192,
          56
        ],
        "example_dtype": "torch.float32",
        "description": "output_tensor_scale"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<c10::BFloat16>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<c10::BFloat16>, std::array<char*, 3ul>)",
    "kernel_implementation": "aten::add",
    "op_mapping": "add",
    "operation": "output = self + other * alpha",
    "source_code": "add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::add <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1024,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "other"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "alpha"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<c10::BFloat16>, std::array<char*, 2ul> >(int, at::native::CUDAFunctorOnSelf_add<c10::BFloat16>, std::array<char*, 2ul>)",
    "kernel_implementation": "aten::add",
    "op_mapping": "add",
    "operation": "output = self + other * alpha",
    "source_code": "add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::add <- sglang/srt/layers/activation.py(77): forward_native <- sglang/srt/layers/activation.py(81): forward_cuda <- sglang/srt/custom_op.py(34): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: NewGELU_0 <- sglang/srt/models/gpt2.py(124): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2MLP_0 <- sglang/srt/models/gpt2.py(161): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Block_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          1536
        ],
        "example_dtype": "c10::BFloat16",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "double",
        "description": "other"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "alpha"
      }
    ]
  },
  {
    "kernel_name": "void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::BFloat16> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::BFloat16> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<c10::BFloat16> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<c10::BFloat16> const&)::{lambda(int)#1})",
    "kernel_implementation": "aten::add",
    "op_mapping": "add",
    "operation": "output = self + other * alpha",
    "source_code": "add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::add <- <built-in method add of type object at 0x7f8c7561fec0> <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1308): fused_experts_impl <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1079): inplace_fused_experts <- sglang::inplace_fused_experts <- <built-in method inplace_fused_experts of PyCapsule object at 0x7f88b221fcf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1236): fused_experts <- sglang/srt/layers/moe/fused_moe_triton/layer.py(156): forward_cuda <- sglang/srt/custom_op.py(34): forward <- sglang/srt/layers/moe/fused_moe_triton/layer.py(120): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(641): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/grok.py(135): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1MoE_0 <- sglang/srt/models/grok.py(350): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1DecoderLayer_0 <- sglang/srt/models/grok.py(437): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Model_0 <- sglang/srt/models/grok.py(572): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          6144
        ],
        "example_dtype": "c10::BFloat16",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1024,
          6144
        ],
        "example_dtype": "c10::BFloat16",
        "description": "other"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "alpha"
      },
      {
        "id": 3,
        "role": "output",
        "example_dim": [
          1024,
          6144
        ],
        "example_dtype": "c10::BFloat16",
        "description": "Output"
      }
    ]
  },
  {
    "kernel_name": "mock_all_gather",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "all_gather",
    "operation": "out = all_gather(rank_i_tensor)",
    "source_code": "",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/mock_dist.py(188): fake_all_gather_into_tensor <- sglang/srt/distributed/parallel_state.py(472): _all_gather_into_tensor <- sglang/srt/distributed/parallel_state.py(144): reg_all_gather_into_tensor <- sglang::reg_all_gather_into_tensor <- <built-in method reg_all_gather_into_tensor of PyCapsule object at 0x734f0034fa50> <- torch/_ops.py(1112): __call__ <- sglang/srt/distributed/parallel_state.py(481): all_gather_into_tensor <- sglang/srt/distributed/parallel_state.py(489): all_gather <- sglang/srt/distributed/communication_op.py(16): tensor_model_parallel_all_gather <- sglang/srt/layers/logits_processor.py(435): _get_logits <- sglang/srt/layers/logits_processor.py(242): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: LogitsProcessor_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "output",
        "example_dim": [
          8,
          6288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          6288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "rank_0_tensor"
      },
      {
        "id": 2,
        "role": "N/A",
        "example_dim": [],
        "example_dtype": "",
        "description": "unrelated"
      }
    ]
  },
  {
    "kernel_name": "ncclDevKernel_AllGather_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",
    "kernel_implementation": "nccl:all_gather",
    "op_mapping": "all_gather",
    "operation": "out = all_gather(rank_i_tensor)",
    "source_code": "https://github.com/pytorch/pytorch/blob/f7130c097efa826313df44f0dcfa7d4d2e4253ec/torch/distributed/distributed_c10d.py#L3891",
    "call_stack": "cudaLaunchKernelExC <- nccl:all_gather <- record_param_comms <- c10d::allgather_ <- torch/distributed/distributed_c10d.py(3637): all_gather <- torch/distributed/c10d_logger.py(78): wrapper <- sglang/srt/distributed/parallel_state.py(489): all_gather <- sglang/srt/layers/dp_attention.py(302): attn_tp_all_gather <- sglang/srt/layers/communicator.py(494): _gather <- sglang/srt/layers/communicator.py(213): postprocess_layer <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- multiprocessing/process.py(108): run <- multiprocessing/process.py(314): _bootstrap <- multiprocessing/spawn.py(129): _main <- multiprocessing/spawn.py(116): spawn_main <- <string>(1): <module>",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          128,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "rank0 tensor to gather"
      }
    ]
  },
  {
    "kernel_name": "ncclDevKernel_Broadcast_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",
    "kernel_implementation": "record_param_comms",
    "op_mapping": "all_gather",
    "operation": "out = all_gather(rank_i_tensor)",
    "source_code": "",
    "call_stack": "cudaLaunchKernelExC <- record_param_comms <- c10d::allgather_ <- torch/distributed/distributed_c10d.py(3637): all_gather <- torch/distributed/c10d_logger.py(78): wrapper <- sglang/srt/distributed/parallel_state.py(489): all_gather <- sglang/srt/layers/dp_attention.py(302): attn_tp_all_gather <- sglang/srt/layers/communicator.py(494): _gather <- sglang/srt/layers/communicator.py(213): postprocess_layer <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- multiprocessing/process.py(108): run <- multiprocessing/process.py(314): _bootstrap <- multiprocessing/spawn.py(129): _main <- multiprocessing/spawn.py(116): spawn_main <- <string>(1): <module>",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          0,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "rank0 tensor to gather"
      }
    ]
  },
  {
    "kernel_name": "mock_all_reduce",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "all_reduce",
    "operation": "out = all_reduce(rank_i_inp)",
    "source_code": "",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/mock_dist.py(157): fake_all_reduce <- sglang/srt/distributed/parallel_state.py(456): _all_reduce_in_place <- sglang/srt/distributed/parallel_state.py(110): inplace_all_reduce <- sglang::inplace_all_reduce <- <built-in method inplace_all_reduce of PyCapsule object at 0x734f0034fa20> <- torch/_ops.py(1112): __call__ <- sglang/srt/distributed/parallel_state.py(398): all_reduce <- sglang/srt/distributed/communication_op.py(11): tensor_model_parallel_all_reduce <- sglang/srt/layers/vocab_parallel_embedding.py(476): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: VocabParallelEmbedding_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "rank_0_inp"
      },
      {
        "id": 1,
        "role": "N/A",
        "example_dim": [],
        "example_dtype": "",
        "description": "unrelated"
      }
    ]
  },
  {
    "kernel_name": "ncclDevKernel_AllReduce_Sum_bf16_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",
    "kernel_implementation": "nccl:all_reduce",
    "op_mapping": "all_reduce",
    "operation": "out = all_reduce(rank_i_inp)",
    "source_code": "https://github.com/pytorch/pytorch/blob/f7130c097efa826313df44f0dcfa7d4d2e4253ec/torch/distributed/distributed_c10d.py#L2835",
    "call_stack": "cudaLaunchKernelExC <- nccl:all_reduce <- record_param_comms <- c10d::allreduce_ <- torch/distributed/distributed_c10d.py(2733): all_reduce <- torch/distributed/c10d_logger.py(78): wrapper <- sglang/srt/distributed/parallel_state.py(456): _all_reduce_in_place <- sglang/srt/distributed/parallel_state.py(110): inplace_all_reduce <- sglang::inplace_all_reduce <- <built-in method inplace_all_reduce of PyCapsule object at 0x7ce0e22e53e0> <- torch/_ops.py(1112): __call__ <- sglang/srt/distributed/parallel_state.py(398): all_reduce <- sglang/srt/distributed/communication_op.py(11): tensor_model_parallel_all_reduce <- sglang/srt/layers/vocab_parallel_embedding.py(476): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: VocabParallelEmbedding_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- multiprocessing/process.py(108): run <- multiprocessing/process.py(314): _bootstrap <- multiprocessing/spawn.py(129): _main <- multiprocessing/spawn.py(116): spawn_main <- <string>(1): <module>",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "rank0 tensor to reduce"
      }
    ]
  },
  {
    "kernel_name": "void sglang::cross_device_reduce_1stage<__nv_bfloat16, 8>(sglang::RankData*, sglang::RankSignals, sglang::Signal*, __nv_bfloat16*, int, int)",
    "kernel_implementation": "sgl_kernel::all_reduce",
    "op_mapping": "all_reduce",
    "operation": "out = all_reduce(rank_i_inp)",
    "source_code": "https://github.com/sgl-project/sglang/blob/489934be0ad3e6335444f589f6e70a0a1f009d68/sgl-kernel/python/sgl_kernel/allreduce.py#L85",
    "call_stack": "cudaLaunchKernel <- sgl_kernel::all_reduce <- <built-in method  of PyCapsule object at 0x70cfd4473990> <- torch/_ops.py(722): __call__ <- sgl_kernel/allreduce.py(64): all_reduce <- sglang/srt/_custom_ops.py(43): all_reduce <- sglang/srt/distributed/device_communicators/custom_all_reduce.py(452): all_reduce <- sglang/srt/distributed/device_communicators/custom_all_reduce.py(475): custom_all_reduce <- sglang/srt/distributed/parallel_state.py(448): _all_reduce_out_place <- sglang/srt/distributed/parallel_state.py(127): outplace_all_reduce <- sglang::outplace_all_reduce <- <built-in method outplace_all_reduce of PyCapsule object at 0x70cfd44736c0> <- torch/_ops.py(1112): __call__ <- sglang/srt/distributed/parallel_state.py(398): all_reduce <- sglang/srt/distributed/communication_op.py(11): tensor_model_parallel_all_reduce <- sglang/srt/layers/vocab_parallel_embedding.py(476): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: VocabParallelEmbedding_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- multiprocessing/process.py(108): run <- multiprocessing/process.py(314): _bootstrap <- multiprocessing/spawn.py(129): _main <- multiprocessing/spawn.py(116): spawn_main <- <string>(1): <module>",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "fa"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "inp"
      },
      {
        "id": 2,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "reg_buffer"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "reg_buffer_sz_bytes"
      }
    ]
  },
  {
    "kernel_name": "void (anonymous namespace)::elementwise_kernel_with_index<int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>(int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}, function_traits<at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>::result_type*)",
    "kernel_implementation": "aten::arange",
    "op_mapping": "arange",
    "operation": "generate tensor with a range of values from start to end with a specified step",
    "source_code": "arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::arange <- aten::arange <- <built-in method arange of type object at 0x7f8c7561fec0> <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1308): fused_experts_impl <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1079): inplace_fused_experts <- sglang::inplace_fused_experts <- <built-in method inplace_fused_experts of PyCapsule object at 0x7f88b221fcf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1236): fused_experts <- sglang/srt/layers/moe/fused_moe_triton/layer.py(156): forward_cuda <- sglang/srt/custom_op.py(34): forward <- sglang/srt/layers/moe/fused_moe_triton/layer.py(120): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(641): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/grok.py(135): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1MoE_0 <- sglang/srt/models/grok.py(350): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1DecoderLayer_0 <- sglang/srt/models/grok.py(437): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Model_0 <- sglang/srt/models/grok.py(572): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "start"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "end"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "step"
      },
      {
        "id": 3,
        "role": "output",
        "example_dim": [
          0
        ],
        "example_dtype": "long int",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::ArgMaxOps<float>, unsigned int, long, 4> >(at::native::ReduceOp<float, at::native::ArgMaxOps<float>, unsigned int, long, 4>)",
    "kernel_implementation": "aten::argmax",
    "op_mapping": "argmax",
    "operation": "out = argmax(self, dim)",
    "source_code": "argmax(Tensor self, int? dim=None, bool keepdim=False) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::argmax <- <built-in method argmax of type object at 0x7352e621fec0> <- sglang/srt/layers/sampler.py(38): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Sampler_0 <- sglang/srt/model_executor/model_runner.py(1254): sample <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          50257
        ],
        "example_dtype": "float",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "dim"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "keepdim"
      }
    ]
  },
  {
    "kernel_name": "void flashinfer::PrefillWithKVCacheKernel<flashinfer::CollectiveMainloop<RaggedParams::AdditionalParams, flashinfer::AttentionKernelTraits<true, 128, 128, 128, 128, 2, cutlass::bfloat16_t, cutlass::bfloat16_t, cutlass::bfloat16_t, int, flashinfer::StandardAttention>, true>, flashinfer::CollectiveEpilogue<flashinfer::AttentionKernelTraits<true, 128, 128, 128, 128, 2, cutlass::bfloat16_t, cutlass::bfloat16_t, cutlass::bfloat16_t, int, flashinfer::StandardAttention> >, flashinfer::AttentionKernelTraits<true, 128, 128, 128, 128, 2, cutlass::bfloat16_t, cutlass::bfloat16_t, cutlass::bfloat16_t, int, flashinfer::StandardAttention>, false, true, flashinfer::BatchPrefillPersistentTileScheduler<int> >(flashinfer::CollectiveMainloop<RaggedParams::AdditionalParams, flashinfer::AttentionKernelTraits<true, 128, 128, 128, 128, 2, cutlass::bfloat16_t, cutlass::bfloat16_t, cutlass::bfloat16_t, int, flashinfer::StandardAttention>, true>::Params, flashinfer::CollectiveEpilogue<flashinfer::AttentionKernelTraits<true, 128, 128, 128, 128, 2, cutlass::bfloat16_t, cutlass::bfloat16_t, cutlass::bfloat16_t, int, flashinfer::StandardAttention> >::Params, flashinfer::BatchPrefillPersistentTileScheduler<int>::Params)",
    "kernel_implementation": "batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False_sm90::ragged_run",
    "op_mapping": "attention",
    "operation": "o = softmax(q@k^T)@v",
    "source_code": "https://github.com/flashinfer-ai/flashinfer/blob/8dd4ed2c25791d6a04ac312a80b6d8294dfc805c/flashinfer/prefill.py#L509",
    "call_stack": "cudaLaunchKernel <- batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False_sm90::ragged_run <- <built-in method  of PyCapsule object at 0x734f002ee220> <- torch/_ops.py(722): __call__ <- flashinfer/prefill.py(206): ragged_run <- flashinfer/prefill.py(2227): run <- flashinfer/prefill.py(2178): forward <- sglang/srt/layers/attention/flashinfer_backend.py(441): forward_extend <- sglang/srt/layers/attention/base_attn_backend.py(57): forward <- sglang/srt/layers/radix_attention.py(83): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RadixAttention_0 <- sglang/srt/models/gpt2.py(84): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Attention_0 <- sglang/srt/models/gpt2.py(161): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Block_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          402653184
        ],
        "example_dtype": "unsigned char",
        "description": "float_workspace_buffer"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          8388608
        ],
        "example_dtype": "unsigned char",
        "description": "int_workspace_buffer"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          8
        ],
        "example_dtype": "long int",
        "description": "plan_info_vec"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          1024,
          12,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "q"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [
          1024,
          12,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "k"
      },
      {
        "id": 5,
        "role": "input",
        "example_dim": [
          1024,
          12,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "v"
      },
      {
        "id": 6,
        "role": "input",
        "example_dim": [
          2
        ],
        "example_dtype": "int",
        "description": "qo_indptr"
      },
      {
        "id": 7,
        "role": "input",
        "example_dim": [
          2
        ],
        "example_dtype": "int",
        "description": "kv_indptr"
      },
      {
        "id": 8,
        "role": "output",
        "example_dim": [
          1024,
          12,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "o"
      },
      {
        "id": 9,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "maybe_lse"
      },
      {
        "id": 10,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "mask_mode"
      },
      {
        "id": 11,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "layout"
      },
      {
        "id": 12,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "window_left"
      },
      {
        "id": 13,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "*args"
      },
      {
        "id": 14,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "*args"
      }
    ]
  },
  {
    "kernel_name": "void flashinfer::BatchDecodeWithPagedKVCacheKernel<(flashinfer::PosEncodingMode)0, 2u, 4u, 8u, 16u, 1u, 8u, flashinfer::DefaultAttention<false, false, false, false>, Params>(Params)",
    "kernel_implementation": "batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False::run",
    "op_mapping": "attention",
    "operation": "o = softmax(q@k^T)@v",
    "source_code": "https://github.com/flashinfer-ai/flashinfer/blob/8dd4ed2c25791d6a04ac312a80b6d8294dfc805c/flashinfer/decode.py#L140",
    "call_stack": "cudaLaunchKernel <- batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False::run <- <built-in method  of PyCapsule object at 0x73302e0c2700> <- torch/_ops.py(722): __call__ <- flashinfer/decode.py(223): run_batch_decode <- flashinfer/decode.py(982): run <- flashinfer/decode.py(929): forward <- sglang/srt/layers/attention/flashinfer_backend.py(517): forward_decode <- sglang/srt/layers/attention/base_attn_backend.py(57): forward <- sglang/srt/layers/radix_attention.py(83): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RadixAttention_0 <- sglang/srt/models/gpt2.py(84): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Attention_0 <- sglang/srt/models/gpt2.py(161): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Block_0 <- sglang/srt/models/gpt2.py(213): forward <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          402653184
        ],
        "example_dtype": "unsigned char",
        "description": "float_workspace_buffer"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          8388608
        ],
        "example_dtype": "unsigned char",
        "description": "int_workspace_buffer"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          10
        ],
        "example_dtype": "long int",
        "description": "plan_info_vec"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          1,
          12,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "q"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [
          21124861,
          1,
          12,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "paged_k_cache"
      },
      {
        "id": 5,
        "role": "input",
        "example_dim": [
          21124861,
          1,
          12,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "paged_v_cache"
      },
      {
        "id": 6,
        "role": "input",
        "example_dim": [
          2
        ],
        "example_dtype": "int",
        "description": "paged_v_cache_indptr"
      },
      {
        "id": 7,
        "role": "input",
        "example_dim": [
          1025
        ],
        "example_dtype": "int",
        "description": "paged_kv_indices"
      },
      {
        "id": 8,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "int",
        "description": "paged_kv_last_page_len."
      },
      {
        "id": 9,
        "role": "output",
        "example_dim": [
          1,
          12,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "o"
      },
      {
        "id": 10,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "maybe_lse"
      },
      {
        "id": 11,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "kv_layout_code"
      },
      {
        "id": 12,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "window_left"
      },
      {
        "id": 13,
        "role": "input",
        "example_dim": [
          12
        ],
        "example_dtype": "float",
        "description": "enable_pdl"
      },
      {
        "id": 14,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "*args"
      },
      {
        "id": 15,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "*args"
      },
      {
        "id": 16,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "*args"
      },
      {
        "id": 17,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "*args"
      }
    ]
  },
  {
    "kernel_name": "void flashinfer::PersistentVariableLengthMergeStatesKernel<8u, 16u, 8u, 4u, __nv_bfloat16, __nv_bfloat16, int>(__nv_bfloat16*, float*, int*, __nv_bfloat16*, float*, unsigned int, unsigned int*, unsigned int)",
    "kernel_implementation": "batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False::run",
    "op_mapping": "attention",
    "operation": "o = softmax(q@k^T)@v",
    "source_code": "https://github.com/flashinfer-ai/flashinfer/blob/8dd4ed2c25791d6a04ac312a80b6d8294dfc805c/flashinfer/decode.py#L140",
    "call_stack": "cudaLaunchKernel <- batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False::run <- <built-in method  of PyCapsule object at 0x73302e0c2700> <- torch/_ops.py(722): __call__ <- flashinfer/decode.py(223): run_batch_decode <- flashinfer/decode.py(982): run <- flashinfer/decode.py(929): forward <- sglang/srt/layers/attention/flashinfer_backend.py(517): forward_decode <- sglang/srt/layers/attention/base_attn_backend.py(57): forward <- sglang/srt/layers/radix_attention.py(83): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RadixAttention_0 <- sglang/srt/models/gpt2.py(84): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Attention_0 <- sglang/srt/models/gpt2.py(161): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Block_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          402653184
        ],
        "example_dtype": "unsigned char",
        "description": "float_workspace_buffer"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          8388608
        ],
        "example_dtype": "unsigned char",
        "description": "int_workspace_buffer"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          10
        ],
        "example_dtype": "long int",
        "description": "plan_info_vec"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          1,
          12,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "q"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [
          21124861,
          1,
          12,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "paged_k_cache"
      },
      {
        "id": 5,
        "role": "input",
        "example_dim": [
          21124861,
          1,
          12,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "paged_v_cache"
      },
      {
        "id": 6,
        "role": "input",
        "example_dim": [
          2
        ],
        "example_dtype": "int",
        "description": "paged_v_cache_indptr"
      },
      {
        "id": 7,
        "role": "input",
        "example_dim": [
          1025
        ],
        "example_dtype": "int",
        "description": "paged_kv_indices"
      },
      {
        "id": 8,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "int",
        "description": "paged_kv_last_page_len."
      },
      {
        "id": 9,
        "role": "output",
        "example_dim": [
          1,
          12,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "o"
      },
      {
        "id": 10,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "maybe_lse"
      },
      {
        "id": 11,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "kv_layout_code"
      },
      {
        "id": 12,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "window_left"
      },
      {
        "id": 13,
        "role": "input",
        "example_dim": [
          12
        ],
        "example_dtype": "float",
        "description": "enable_pdl"
      },
      {
        "id": 14,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "*args"
      },
      {
        "id": 15,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "*args"
      },
      {
        "id": 16,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "*args"
      },
      {
        "id": 17,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "*args"
      }
    ]
  },
  {
    "kernel_name": "void flashinfer::PrefillWithKVCacheKernel<flashinfer::CollectiveMainloop<RaggedParams::AdditionalParams, flashinfer::AttentionKernelTraits<true, 192, 128, 128, 128, 2, cutlass::bfloat16_t, cutlass::bfloat16_t, cutlass::bfloat16_t, int, flashinfer::StandardAttention>, true>, flashinfer::CollectiveEpilogue<flashinfer::AttentionKernelTraits<true, 192, 128, 128, 128, 2, cutlass::bfloat16_t, cutlass::bfloat16_t, cutlass::bfloat16_t, int, flashinfer::StandardAttention> >, flashinfer::AttentionKernelTraits<true, 192, 128, 128, 128, 2, cutlass::bfloat16_t, cutlass::bfloat16_t, cutlass::bfloat16_t, int, flashinfer::StandardAttention>, false, true, flashinfer::BatchPrefillPersistentTileScheduler<int> >(flashinfer::CollectiveMainloop<RaggedParams::AdditionalParams, flashinfer::AttentionKernelTraits<true, 192, 128, 128, 128, 2, cutlass::bfloat16_t, cutlass::bfloat16_t, cutlass::bfloat16_t, int, flashinfer::StandardAttention>, true>::Params, flashinfer::CollectiveEpilogue<flashinfer::AttentionKernelTraits<true, 192, 128, 128, 128, 2, cutlass::bfloat16_t, cutlass::bfloat16_t, cutlass::bfloat16_t, int, flashinfer::StandardAttention> >::Params, flashinfer::BatchPrefillPersistentTileScheduler<int>::Params)",
    "kernel_implementation": "batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_192_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False_sm90::ragged_run",
    "op_mapping": "attention",
    "operation": "o = softmax(q@k^T)@v",
    "source_code": "https://github.com/flashinfer-ai/flashinfer/blob/8dd4ed2c25791d6a04ac312a80b6d8294dfc805c/flashinfer/prefill.py#L509",
    "call_stack": "cudaLaunchKernel <- batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_192_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False_sm90::ragged_run <- <built-in method  of PyCapsule object at 0x785a346efcc0> <- torch/_ops.py(722): __call__ <- flashinfer/prefill.py(206): ragged_run <- flashinfer/prefill.py(2227): run <- flashinfer/prefill.py(2178): forward <- sglang/srt/layers/attention/flashinfer_mla_backend.py(366): forward_extend <- sglang/srt/layers/attention/base_attn_backend.py(57): forward <- sglang/srt/layers/radix_attention.py(83): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RadixAttention_0 <- sglang/srt/models/deepseek_v2.py(899): forward_normal_core <- sglang/srt/models/deepseek_v2.py(837): forward_core <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "input",
        "example_dim": [
          402653184
        ],
        "example_dtype": "unsigned char",
        "description": "float_workspace_buffer"
      },
      {
        "role": "input",
        "example_dim": [
          8388608
        ],
        "example_dtype": "unsigned char",
        "description": "int_workspace_buffer"
      },
      {
        "role": "input",
        "example_dim": [
          8
        ],
        "example_dtype": "long int",
        "description": "plan_info_vec"
      },
      {
        "role": "input",
        "example_dim": [
          1024,
          16,
          192
        ],
        "example_dtype": "c10::BFloat16",
        "description": "q"
      },
      {
        "role": "input",
        "example_dim": [
          1024,
          16,
          192
        ],
        "example_dtype": "c10::BFloat16",
        "description": "k"
      },
      {
        "role": "input",
        "example_dim": [
          1024,
          16,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "v"
      },
      {
        "role": "input",
        "example_dim": [
          2
        ],
        "example_dtype": "int",
        "description": "qo_indptr"
      },
      {
        "role": "input",
        "example_dim": [
          2
        ],
        "example_dtype": "int",
        "description": "kv_indptr"
      },
      {
        "role": "output",
        "example_dim": [
          1024,
          16,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "o"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "maybe_lse"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "mask_mode"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "layout"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "window_left"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "*args"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "*args"
      }
    ]
  },
  {
    "kernel_name": "void flashinfer::mla::hopper::BatchMLAPageAttentionHopperKernel<flashinfer::mla::hopper::HopperKernelTraits<false, 2u, 512u, 64u, 64u, 64u, __nv_bfloat16, __nv_bfloat16, __nv_bfloat16, int>, flashinfer::MLAParams<__nv_bfloat16, __nv_bfloat16, __nv_bfloat16, int> >(flashinfer::MLAParams<__nv_bfloat16, __nv_bfloat16, __nv_bfloat16, int>)",
    "kernel_implementation": "batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False_sm90::run",
    "op_mapping": "attention",
    "operation": "o = softmax(q@k^T)@v",
    "source_code": "https://github.com/flashinfer-ai/flashinfer/blob/3b01face12e6520abdb3ac6ac8a5797ec4521c49/flashinfer/mla.py#L419",
    "call_stack": "cudaLaunchCooperativeKernel <- batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False_sm90::run <- <built-in method  of PyCapsule object at 0x783aa2b1dc80> <- torch/_ops.py(722): __call__ <- flashinfer/mla.py(287): run <- sglang/srt/layers/attention/flashinfer_mla_backend.py(435): forward_decode <- sglang/srt/layers/attention/base_attn_backend.py(57): forward <- sglang/srt/layers/radix_attention.py(83): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RadixAttention_2 <- sglang/srt/models/deepseek_v2.py(982): forward_absorb_core <- sglang/srt/models/deepseek_v2.py(837): forward_core <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          402653184
        ],
        "example_dtype": "unsigned char",
        "description": "_float_workspace_buffer"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          8388608
        ],
        "example_dtype": "unsigned char",
        "description": "_int_workspace_buffer"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          18
        ],
        "example_dtype": "long int",
        "description": "_plan_info"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          1,
          16,
          512
        ],
        "example_dtype": "c10::BFloat16",
        "description": "q_nope"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [
          1,
          16,
          64
        ],
        "example_dtype": "c10::BFloat16",
        "description": "q_pe"
      },
      {
        "id": 5,
        "role": "input",
        "example_dim": [
          55622988,
          1,
          512
        ],
        "example_dtype": "c10::BFloat16",
        "description": "ckv_cache"
      },
      {
        "id": 6,
        "role": "input",
        "example_dim": [
          55622988,
          1,
          64
        ],
        "example_dtype": "c10::BFloat16",
        "description": "kpe_cache"
      },
      {
        "id": 7,
        "role": "input",
        "example_dim": [
          1025
        ],
        "example_dtype": "int",
        "description": "_kv_indices_buf"
      },
      {
        "id": 8,
        "role": "output",
        "example_dim": [
          1,
          16,
          512
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      },
      {
        "id": 9,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "lse"
      },
      {
        "id": 10,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "mask_mode"
      },
      {
        "id": 11,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "num_heads"
      },
      {
        "id": 12,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "page_size"
      },
      {
        "id": 13,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "sm_scale"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<4, at::native::bitwise_not_kernel_cuda(at::TensorIteratorBase&)::{lambda(bool)#1}, std::array<char*, 2ul> >(int, at::native::bitwise_not_kernel_cuda(at::TensorIteratorBase&)::{lambda(bool)#1}, std::array<char*, 2ul>)",
    "kernel_implementation": "aten::bitwise_not",
    "op_mapping": "bitwise_not",
    "operation": "out = bitwise_not(self)",
    "source_code": "bitwise_not(Tensor self) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::bitwise_not <- get_masked_input_and_mask <- sglang/srt/layers/vocab_parallel_embedding.py(476): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: VocabParallelEmbedding_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024
        ],
        "example_dtype": "long int",
        "description": "self"
      },
      {
        "id": 1,
        "role": "output",
        "example_dim": [
          1024
        ],
        "example_dtype": "bool",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void gemv2T_kernel_val<int, int, __nv_bfloat16, __nv_bfloat16, __nv_bfloat16, float, 128, 16, 2, 2, false, false, cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<__nv_bfloat16 const>, cublasGemvTensorStridedBatched<__nv_bfloat16 const>, cublasGemvTensorStridedBatched<__nv_bfloat16>, float> >(cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<__nv_bfloat16 const>, cublasGemvTensorStridedBatched<__nv_bfloat16 const>, cublasGemvTensorStridedBatched<__nv_bfloat16>, float>, float, float)",
    "kernel_implementation": "aten::bmm",
    "op_mapping": "bmm",
    "operation": "C = A dot B",
    "source_code": "",
    "call_stack": "cudaLaunchKernel <- aten::bmm <- <built-in method bmm of type object at 0x785e1b21fec0> <- sglang/srt/models/deepseek_v2.py(905): forward_absorb_prepare <- sglang/srt/models/deepseek_v2.py(802): forward_prepare <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          16,
          1,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "A"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          16,
          128,
          512
        ],
        "example_dtype": "c10::BFloat16",
        "description": "B"
      }
    ]
  },
  {
    "kernel_name": "void cutlass::Kernel2<cutlass_80_wmma_tensorop_bf16_s161616gemm_bf16_16x16_128x1_tn_align8>(cutlass_80_wmma_tensorop_bf16_s161616gemm_bf16_16x16_128x1_tn_align8::Params)",
    "kernel_implementation": "aten::bmm",
    "op_mapping": "bmm",
    "operation": "C = A dot B",
    "source_code": "",
    "call_stack": "cuLaunchKernel <- aten::bmm <- <built-in method bmm of type object at 0x730f2e81fec0> <- sglang/srt/models/deepseek_v2.py(905): forward_absorb_prepare <- sglang/srt/models/deepseek_v2.py(802): forward_prepare <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          128,
          1,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "A=[B, M, K]"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          128,
          128,
          512
        ],
        "example_dtype": "c10::BFloat16",
        "description": "B=[B, K, N]"
      }
    ]
  },
  {
    "kernel_name": "void at::native::(anonymous namespace)::CatArrayBatchedCopy_contig<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 2, 128, 1>(at::native::(anonymous namespace)::OpaqueType<2u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<2u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",
    "kernel_implementation": "aten::cat",
    "op_mapping": "cat",
    "operation": "Concatenates a list of tensors along a specified dimension.",
    "source_code": "",
    "call_stack": "cudaLaunchKernel <- aten::cat <- <built-in method cat of type object at 0x7352e621fec0> <- sglang/mock_dist.py(188): fake_all_gather_into_tensor <- sglang/srt/distributed/parallel_state.py(472): _all_gather_into_tensor <- sglang/srt/distributed/parallel_state.py(144): reg_all_gather_into_tensor <- sglang::reg_all_gather_into_tensor <- <built-in method reg_all_gather_into_tensor of PyCapsule object at 0x734f0034fa50> <- torch/_ops.py(1112): __call__ <- sglang/srt/distributed/parallel_state.py(481): all_gather_into_tensor <- sglang/srt/distributed/parallel_state.py(489): all_gather <- sglang/srt/distributed/communication_op.py(16): tensor_model_parallel_all_gather <- sglang/srt/layers/logits_processor.py(435): _get_logits <- sglang/srt/layers/logits_processor.py(242): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: LogitsProcessor_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          [
            1,
            6288
          ],
          [
            1,
            6288
          ],
          [
            1,
            6288
          ],
          [
            1,
            6288
          ],
          [
            1,
            6288
          ],
          [
            1,
            6288
          ],
          [
            1,
            6288
          ],
          [
            1,
            6288
          ]
        ],
        "example_dtype": "TensorList",
        "description": "List of input tensors to concatenate. Each tensor in the list is expected to have the same shape except in the concatenation dimension."
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "Dimension along which to concatenate the tensors. This is typically an integer scalar specifying the axis."
      }
    ]
  },
  {
    "kernel_name": "void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}, std::array<char*, 2ul>, 4, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithCast<1>, at::native::memory::StoreWithCast<1> >(int, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}, std::array<char*, 2ul>, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithCast<1>, at::native::memory::StoreWithCast<1>)",
    "kernel_implementation": "aten::copy_",
    "op_mapping": "copy_",
    "operation": "self = copy(src)",
    "source_code": "copy(Tensor self, Tensor src, bool non_blocking=False) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::copy_ <- aten::_to_copy <- aten::to <- aten::sum <- aten::sum <- <built-in method sum of Tensor object at 0x73302e12a4d0> <- sglang/srt/layers/attention/flashinfer_backend.py(786): update_single_wrapper <- sglang/srt/layers/attention/flashinfer_backend.py(197): init_forward_metadata <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "output",
        "example_dim": [
          1
        ],
        "example_dtype": "long int",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "int",
        "description": "src"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "non_blocking"
      }
    ]
  },
  {
    "kernel_name": "void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#3}::operator()() const::{lambda(int)#1}, std::array<char*, 2ul>, 4, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithCast<1>, at::native::memory::StoreWithCast<1> >(int, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#3}::operator()() const::{lambda(int)#1}, std::array<char*, 2ul>, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithCast<1>, at::native::memory::StoreWithCast<1>)",
    "kernel_implementation": "aten::copy_",
    "op_mapping": "copy_",
    "operation": "self = copy(src)",
    "source_code": "copy(Tensor self, Tensor src, bool non_blocking=False) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::copy_ <- sglang/srt/layers/attention/flashinfer_backend.py(898): call_begin_forward <- sglang/srt/layers/attention/flashinfer_backend.py(786): update_single_wrapper <- sglang/srt/layers/attention/flashinfer_backend.py(197): init_forward_metadata <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "output",
        "example_dim": [
          1
        ],
        "example_dtype": "long int",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "int",
        "description": "src"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "non_blocking"
      }
    ]
  },
  {
    "kernel_name": "void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#12}::operator()() const::{lambda(c10::BFloat16)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#12}::operator()() const::{lambda(c10::BFloat16)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#12}::operator()() const::{lambda(c10::BFloat16)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#12}::operator()() const::{lambda(c10::BFloat16)#1} const&)::{lambda(int)#1})",
    "kernel_implementation": "aten::copy_",
    "op_mapping": "copy_",
    "operation": "self = copy(src)",
    "source_code": "copy(Tensor self, Tensor src, bool non_blocking=False) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::copy_ <- aten::clone <- aten::contiguous <- <built-in method contiguous of Tensor object at 0x73302e12a700> <- sglang/srt/layers/attention/flashinfer_backend.py(441): forward_extend <- sglang/srt/layers/attention/base_attn_backend.py(57): forward <- sglang/srt/layers/radix_attention.py(83): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RadixAttention_0 <- sglang/srt/models/gpt2.py(84): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Attention_0 <- sglang/srt/models/gpt2.py(161): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Block_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "output",
        "example_dim": [
          1024,
          1536
        ],
        "example_dtype": "c10::BFloat16",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1024,
          1536
        ],
        "example_dtype": "c10::BFloat16",
        "description": "src"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "non_blocking"
      }
    ]
  },
  {
    "kernel_name": "void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul>, 4, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithCast<1>, at::native::memory::StoreWithCast<1> >(int, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, std::array<char*, 2ul>, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithCast<1>, at::native::memory::StoreWithCast<1>)",
    "kernel_implementation": "aten::copy_",
    "op_mapping": "copy_",
    "operation": "self = copy(src)",
    "source_code": "copy(Tensor self, Tensor src, bool non_blocking=False) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::copy_ <- aten::_to_copy <- aten::to <- <built-in method float of Tensor object at 0x73302e117880> <- sglang/srt/layers/logits_processor.py(435): _get_logits <- sglang/srt/layers/logits_processor.py(242): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: LogitsProcessor_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "output",
        "example_dim": [
          1,
          50257
        ],
        "example_dtype": "float",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          50257
        ],
        "example_dtype": "c10::BFloat16",
        "description": "src"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "non_blocking"
      }
    ]
  },
  {
    "kernel_name": "void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1} const&)::{lambda(int)#1})",
    "kernel_implementation": "aten::copy_",
    "op_mapping": "copy_",
    "operation": "self = copy(src)",
    "source_code": "copy(Tensor self, Tensor src, bool non_blocking=False) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::copy_ <- aten::repeat <- <built-in method repeat of Tensor object at 0x7f68f1cf7f10> <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1308): fused_experts_impl <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1079): inplace_fused_experts <- sglang::inplace_fused_experts <- <built-in method inplace_fused_experts of PyCapsule object at 0x7f88b221fcf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1236): fused_experts <- sglang/srt/layers/moe/fused_moe_triton/layer.py(156): forward_cuda <- sglang/srt/custom_op.py(34): forward <- sglang/srt/layers/moe/fused_moe_triton/layer.py(120): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(641): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/grok.py(135): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1MoE_0 <- sglang/srt/models/grok.py(350): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1DecoderLayer_0 <- sglang/srt/models/grok.py(437): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Model_0 <- sglang/srt/models/grok.py(572): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "output",
        "example_dim": [
          256,
          8
        ],
        "example_dtype": "long int",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          256,
          8
        ],
        "example_dtype": "long int",
        "description": "src"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "non_blocking"
      }
    ]
  },
  {
    "kernel_name": "void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::float8_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda(c10::Float8_e4m3fn)#1}>(at::TensorIteratorBase&, at::native::float8_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda(c10::Float8_e4m3fn)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::float8_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda(c10::Float8_e4m3fn)#1}>(at::TensorIteratorBase&, at::native::float8_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda(c10::Float8_e4m3fn)#1} const&)::{lambda(int)#1})",
    "kernel_implementation": "aten::copy_",
    "op_mapping": "copy_",
    "operation": "self = copy(src)",
    "source_code": "copy(Tensor self, Tensor src, bool non_blocking=False) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::copy_ <- aten::repeat <- <built-in method repeat of Tensor object at 0x7ad0eaad12b0> <- sglang/mock_dist.py(422): fake_dispatch <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(300): _dispatch_core <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(241): dispatch_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(711): dispatch_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(691): dispatch <- sglang/srt/two_batch_overlap.py(624): _execute <- sglang/srt/two_batch_overlap.py(627): dispatch <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "output",
        "example_dim": [
          8,
          1,
          128,
          7168
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "self"
      },
      {
        "role": "input",
        "example_dim": [
          8,
          1,
          128,
          7168
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "src"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "non_blocking"
      }
    ]
  },
  {
    "kernel_name": "void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",
    "kernel_implementation": "aten::copy_",
    "op_mapping": "copy_",
    "operation": "Performs elementwise direct copy operation for float tensors.",
    "source_code": "",
    "call_stack": "cudaLaunchKernel <- aten::copy_ <- aten::repeat <- <built-in method repeat of Tensor object at 0x7ad0eaad12b0> <- sglang/mock_dist.py(422): fake_dispatch <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(300): _dispatch_core <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(241): dispatch_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(711): dispatch_b <- sglang/srt/layers/moe/ep_moe/token_dispatcher.py(691): dispatch <- sglang/srt/two_batch_overlap.py(624): _execute <- sglang/srt/two_batch_overlap.py(627): dispatch <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "input",
        "example_dim": [
          8,
          1,
          128,
          56
        ],
        "example_dtype": "float",
        "description": "Source tensor to be copied."
      },
      {
        "role": "output",
        "example_dim": [
          8,
          1,
          128,
          56
        ],
        "example_dtype": "float",
        "description": "Destination tensor where values are copied to."
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "Optional scalar parameter for kernel configuration."
      }
    ]
  },
  {
    "kernel_name": "void at_cuda_detail::cub::DeviceScanInitKernel<at_cuda_detail::cub::ScanTileState<long, true> >(at_cuda_detail::cub::ScanTileState<long, true>, int)",
    "kernel_implementation": "aten::cumsum",
    "op_mapping": "cumsum",
    "operation": "out = cumsum(self, dim)",
    "source_code": "cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::cumsum <- <built-in method cumsum of type object at 0x7352e621fec0> <- sglang/srt/layers/attention/flashinfer_backend.py(898): call_begin_forward <- sglang/srt/layers/attention/flashinfer_backend.py(786): update_single_wrapper <- sglang/srt/layers/attention/flashinfer_backend.py(197): init_forward_metadata <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "int",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "dim"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "type"
      }
    ]
  },
  {
    "kernel_name": "void at_cuda_detail::cub::DeviceScanKernel<at_cuda_detail::cub::DeviceScanPolicy<long, std::plus<long> >::Policy900, long const*, long*, at_cuda_detail::cub::ScanTileState<long, true>, std::plus<long>, at_cuda_detail::cub::NullType, int, long>(long const*, long*, at_cuda_detail::cub::ScanTileState<long, true>, int, std::plus<long>, at_cuda_detail::cub::NullType, int)",
    "kernel_implementation": "aten::cumsum",
    "op_mapping": "cumsum",
    "operation": "out = cumsum(self, dim)",
    "source_code": "cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::cumsum <- <built-in method cumsum of type object at 0x7352e621fec0> <- sglang/srt/layers/attention/flashinfer_backend.py(898): call_begin_forward <- sglang/srt/layers/attention/flashinfer_backend.py(786): update_single_wrapper <- sglang/srt/layers/attention/flashinfer_backend.py(197): init_forward_metadata <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "int",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "dim"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "type"
      }
    ]
  },
  {
    "kernel_name": "void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::DivFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::DivFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::DivFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::DivFunctor<float> > const&)::{lambda(int)#1})",
    "kernel_implementation": "aten::div",
    "op_mapping": "div",
    "operation": "out = self / other",
    "source_code": "div.Tensor(Tensor self, Tensor other) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::div <- sglang/srt/layers/moe/topk.py(64): fused_topk <- sglang/srt/layers/moe/topk.py(298): select_experts <- sglang/srt/layers/moe/fused_moe_triton/layer.py(156): forward_cuda <- sglang/srt/custom_op.py(34): forward <- sglang/srt/layers/moe/fused_moe_triton/layer.py(120): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(641): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/qwen3_moe.py(168): forward_normal <- sglang/srt/models/qwen3_moe.py(152): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeSparseMoeBlock_0 <- sglang/srt/models/qwen3_moe.py(559): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeDecoderLayer_0 <- sglang/srt/models/qwen2_moe.py(431): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeModel_0 <- sglang/srt/models/qwen3_moe.py(690): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "input",
        "example_dim": [
          1024,
          8
        ],
        "example_dtype": "float",
        "description": "self"
      },
      {
        "role": "input",
        "example_dim": [
          1024,
          1
        ],
        "example_dtype": "float",
        "description": "other"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<c10::BFloat16>, std::array<char*, 1ul> >(int, at::native::FillFunctor<c10::BFloat16>, std::array<char*, 1ul>)",
    "kernel_implementation": "aten::fill_",
    "op_mapping": "fill_",
    "operation": "self = fill(value)",
    "source_code": "fill.Scalar(Tensor self, Scalar value) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::fill_ <- aten::zero_ <- aten::zeros <- <built-in method zeros of type object at 0x7352e621fec0> <- sglang/srt/model_executor/forward_batch_info.py(261): init_new <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "output",
        "example_dim": [
          1024,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "value"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<int>, std::array<char*, 1ul> >(int, at::native::FillFunctor<int>, std::array<char*, 1ul>)",
    "kernel_implementation": "aten::fill_",
    "op_mapping": "fill_",
    "operation": "self = fill(value)",
    "source_code": "fill.Scalar(Tensor self, Scalar value) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::fill_ <- <built-in method fill_ of Tensor object at 0x7f68f1cf6340> <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(653): moe_align_block_size <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1308): fused_experts_impl <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1079): inplace_fused_experts <- sglang::inplace_fused_experts <- <built-in method inplace_fused_experts of PyCapsule object at 0x7f88b221fcf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1236): fused_experts <- sglang/srt/layers/moe/fused_moe_triton/layer.py(156): forward_cuda <- sglang/srt/custom_op.py(34): forward <- sglang/srt/layers/moe/fused_moe_triton/layer.py(120): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(641): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/grok.py(135): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1MoE_0 <- sglang/srt/models/grok.py(350): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1DecoderLayer_0 <- sglang/srt/models/grok.py(437): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Model_0 <- sglang/srt/models/grok.py(572): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input/output",
        "example_dim": [
          3064
        ],
        "example_dtype": "int",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "value"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, std::array<char*, 1ul> >(int, at::native::FillFunctor<float>, std::array<char*, 1ul>)",
    "kernel_implementation": "aten::fill_",
    "op_mapping": "fill_",
    "operation": "self = fill(value)",
    "source_code": "fill.Scalar(Tensor self, Scalar value) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::fill_ <- aten::zero_ <- aten::zeros <- <built-in method zeros of type object at 0x785e1b21fec0> <- sglang/srt/utils.py(2137): __init__ <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "input/output",
        "example_dim": [
          4
        ],
        "example_dtype": "float",
        "description": "self"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "value"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<long>, std::array<char*, 1ul> >(int, at::native::FillFunctor<long>, std::array<char*, 1ul>)",
    "kernel_implementation": "aten::fill_",
    "op_mapping": "fill_",
    "operation": "self = fill(value)",
    "source_code": "fill.Scalar(Tensor self, Scalar value) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::fill_ <- aten::zero_ <- aten::zeros_like <- <built-in method zeros_like of type object at 0x730f2e81fec0> <- sglang/srt/layers/dp_attention.py(166): get_dp_local_info <- sglang/srt/layers/dp_attention.py(224): _dp_gather <- sglang/srt/layers/dp_attention.py(258): dp_gather_partial <- sglang/srt/layers/communicator.py(362): _gather_hidden_states <- sglang/srt/layers/communicator.py(199): prepare_mlp <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input/output",
        "example_dim": [],
        "example_dtype": "long int",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "value"
      }
    ]
  },
  {
    "kernel_name": "void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<2> >(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char const*, long)#1}>(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<2> >(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char const*, long)#1} const&)::{lambda(int)#1}>(long, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<2> >(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char const*, long)#1}>(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<2> >(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char const*, long)#1} const&)::{lambda(int)#1})",
    "kernel_implementation": "aten::index",
    "op_mapping": "index",
    "operation": "out = index(self, indices)",
    "source_code": "index.Tensor(Tensor self, Tensor?[] indices) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::index <- sglang/srt/layers/logits_processor.py(242): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: LogitsProcessor_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "indices"
      }
    ]
  },
  {
    "kernel_name": "void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char const*, long)#1}>(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char const*, long)#1} const&)::{lambda(int)#1}>(long, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char const*, long)#1}>(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char const*, long)#1} const&)::{lambda(int)#1})",
    "kernel_implementation": "aten::index",
    "op_mapping": "index",
    "operation": "out = index(self, indices)",
    "source_code": "index.Tensor(Tensor self, Tensor?[] indices) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::index <- sglang/srt/managers/expert_location_dispatch.py(88): _topk_ids_logical_to_physical_static <- sglang/srt/managers/expert_location_dispatch.py(75): topk_ids_logical_to_physical <- sglang/srt/layers/moe/topk.py(233): biased_grouped_topk <- sglang/srt/layers/moe/topk.py(298): select_experts <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "input",
        "example_dim": [
          256
        ],
        "example_dtype": "long int",
        "description": "self"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "indices"
      }
    ]
  },
  {
    "kernel_name": "void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_put_kernel_impl<at::native::OpaqueType<2> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char const*, long)#1}>(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_put_kernel_impl<at::native::OpaqueType<2> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char const*, long)#1} const&)::{lambda(int)#1}>(long, at::native::gpu_index_kernel<at::native::index_put_kernel_impl<at::native::OpaqueType<2> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char const*, long)#1}>(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_put_kernel_impl<at::native::OpaqueType<2> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char const*, long)#1} const&)::{lambda(int)#1})",
    "kernel_implementation": "aten::_index_put_impl_",
    "op_mapping": "index_put_",
    "operation": "out = index_put(self, indices, values)",
    "source_code": "index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::_index_put_impl_ <- aten::index_put_ <- sglang/srt/mem_cache/memory_pool.py(378): set_kv_buffer <- sglang/srt/layers/attention/flashinfer_backend.py(441): forward_extend <- sglang/srt/layers/attention/base_attn_backend.py(57): forward <- sglang/srt/layers/radix_attention.py(83): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RadixAttention_0 <- sglang/srt/models/gpt2.py(84): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Attention_0 <- sglang/srt/models/gpt2.py(161): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Block_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          21124861,
          12,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "indices"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          1024,
          12,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "values"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "accumulate"
      },
      {
        "id": 4,
        "role": "N/A",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "unrelated"
      }
    ]
  },
  {
    "kernel_name": "void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_put_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char const*, long)#1}>(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_put_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char const*, long)#1} const&)::{lambda(int)#1}>(long, at::native::gpu_index_kernel<at::native::index_put_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char const*, long)#1}>(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_put_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char const*, long)#1} const&)::{lambda(int)#1})",
    "kernel_implementation": "aten::_index_put_impl_",
    "op_mapping": "index_put_",
    "operation": "out = index_put(self, indices, values)",
    "source_code": "index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::_index_put_impl_ <- aten::index_put_ <- sglang/srt/mem_cache/memory_pool.py(77): write <- sglang/srt/managers/schedule_batch.py(1466): prepare_for_decode <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "output",
        "example_dim": [
          4096,
          2052
        ],
        "example_dtype": "int",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "indices"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "int",
        "description": "values"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "accumulate"
      },
      {
        "id": 4,
        "role": "N/A",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "unrelated"
      }
    ]
  },
  {
    "kernel_name": "void at::native::(anonymous namespace)::indexSelectLargeIndex<c10::BFloat16, long, unsigned int, 2, 2, -2, true>(at::cuda::detail::TensorInfo<c10::BFloat16, unsigned int>, at::cuda::detail::TensorInfo<c10::BFloat16 const, unsigned int>, at::cuda::detail::TensorInfo<long const, unsigned int>, int, int, unsigned int, unsigned int, long)",
    "kernel_implementation": "aten::index_select",
    "op_mapping": "index_select",
    "operation": "out = index_select(self, dim, index)",
    "source_code": "index_select(Tensor self, int dim, Tensor index) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::index_select <- aten::embedding <- <built-in method embedding of type object at 0x7352e621fec0> <- torch/nn/functional.py(2437): embedding <- sglang/srt/layers/vocab_parallel_embedding.py(62): embedding <- sglang/srt/layers/vocab_parallel_embedding.py(476): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: VocabParallelEmbedding_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          6288,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "dim"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          1024
        ],
        "example_dtype": "long int",
        "description": "index"
      }
    ]
  },
  {
    "kernel_name": "void at::native::(anonymous namespace)::indexSelectSmallIndex<c10::BFloat16, long, unsigned int, 2, 2, -2>(at::cuda::detail::TensorInfo<c10::BFloat16, unsigned int>, at::cuda::detail::TensorInfo<c10::BFloat16 const, unsigned int>, at::cuda::detail::TensorInfo<long const, unsigned int>, int, int, unsigned int, long)",
    "kernel_implementation": "aten::index_select",
    "op_mapping": "index_select",
    "operation": "out = index_select(self, dim, index)",
    "source_code": "index_select(Tensor self, int dim, Tensor index) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::index_select <- aten::embedding <- <built-in method embedding of type object at 0x7352e621fec0> <- torch/nn/functional.py(2437): embedding <- sglang/srt/layers/vocab_parallel_embedding.py(62): embedding <- sglang/srt/layers/vocab_parallel_embedding.py(476): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: VocabParallelEmbedding_0 <- sglang/srt/models/gpt2.py(213): forward <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          6288,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "dim"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "long int",
        "description": "index"
      }
    ]
  },
  {
    "kernel_name": "void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<c10::BFloat16, float>(int, float, c10::BFloat16 const*, c10::BFloat16 const*, c10::BFloat16 const*, float*, float*, c10::BFloat16*)",
    "kernel_implementation": "aten::native_layer_norm",
    "op_mapping": "layer_norm",
    "operation": "native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps)",
    "source_code": "native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)",
    "call_stack": "cudaLaunchKernel <- aten::native_layer_norm <- aten::layer_norm <- <built-in method layer_norm of type object at 0x7352e621fec0> <- torch/nn/functional.py(2889): layer_norm <- torch/nn/modules/normalization.py(216): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: LayerNorm_0 <- sglang/srt/models/gpt2.py(161): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Block_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "input"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "ScalarList",
        "description": "normalized_shape"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "weight"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "bias"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "epsilon"
      }
    ]
  },
  {
    "kernel_name": "void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl_nocast<at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#12}::operator()() const::{lambda(c10::BFloat16, bool)#1}>(at::TensorIteratorBase&, at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#12}::operator()() const::{lambda(c10::BFloat16, bool)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#12}::operator()() const::{lambda(c10::BFloat16, bool)#1}>(at::TensorIteratorBase&, at::native::(anonymous namespace)::masked_fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#12}::operator()() const::{lambda(c10::BFloat16, bool)#1} const&)::{lambda(int)#1})",
    "kernel_implementation": "aten::masked_fill_",
    "op_mapping": "masked_fill_",
    "operation": "masked_fill(tensor, mask, value)",
    "source_code": "Tensor.masked_fill_(mask, value)",
    "call_stack": "cudaLaunchKernel <- aten::masked_fill_ <- <built-in method masked_fill_ of Tensor object at 0x734f566bc130> <- sglang/srt/layers/vocab_parallel_embedding.py(476): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: VocabParallelEmbedding_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input/output",
        "example_dim": [
          1024,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "tensor"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1024,
          1
        ],
        "example_dtype": "bool",
        "description": "mask"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "value"
      }
    ]
  },
  {
    "kernel_name": "_w8a8_block_fp8_matmul",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "matmul",
    "operation": "",
    "source_code": "",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(443): __call__ <- triton/runtime/jit.py(563): run <- triton/runtime/jit.py(330): <lambda> <- sglang/srt/layers/quantization/fp8_utils.py(284): triton_w8a8_block_fp8_linear <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(285): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: ReplicatedLinear_0 <- sglang/srt/models/deepseek_v2.py(855): forward_normal_prepare <- sglang/srt/models/deepseek_v2.py(802): forward_prepare <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "input",
        "example_dim": [
          1024,
          7168
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "q_input"
      },
      {
        "role": "input",
        "example_dim": [
          2112,
          7168
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "weight"
      }
    ]
  },
  {
    "kernel_name": "sm90_xmma_gemm_bf16bf16_bf16f32_f32_tn_n_tilesize128x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas",
    "kernel_implementation": "aten::addmm",
    "op_mapping": "matmul",
    "operation": "out = alpha*mat1 @ mat2 + beta*bias",
    "source_code": "addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor",
    "call_stack": "cudaLaunchKernelExC <- aten::addmm <- aten::linear <- <built-in function linear> <- sglang/srt/layers/linear.py(168): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: QKVParallelLinear_0 <- sglang/srt/models/gpt2.py(84): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Attention_0 <- sglang/srt/models/gpt2.py(161): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Block_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "bias"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          1536
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          1536,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "beta"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "alpha"
      }
    ]
  },
  {
    "kernel_name": "sm90_xmma_gemm_bf16bf16_bf16f32_f32_tn_n_tilesize64x64x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas",
    "kernel_implementation": "aten::mm",
    "op_mapping": "matmul",
    "operation": "out = mat1 @ mat2",
    "source_code": "mm(Tensor self, Tensor mat2) -> Tensor",
    "call_stack": "cudaLaunchKernelExC <- aten::mm <- aten::matmul <- <built-in method matmul of type object at 0x7352e621fec0> <- sglang/srt/layers/logits_processor.py(435): _get_logits <- sglang/srt/layers/logits_processor.py(242): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: LogitsProcessor_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          12288,
          6288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      }
    ]
  },
  {
    "kernel_name": "void cutlass::Kernel2<cutlass_80_tensorop_bf16_s16816gemm_relu_bf16_128x64_64x4_tn_align8>(cutlass_80_tensorop_bf16_s16816gemm_relu_bf16_128x64_64x4_tn_align8::Params)",
    "kernel_implementation": "aten::addmm",
    "op_mapping": "matmul",
    "operation": "out = alpha*mat1 @ mat2 + beta*bias",
    "source_code": "addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor",
    "call_stack": "cuLaunchKernel <- aten::addmm <- aten::linear <- <built-in function linear> <- sglang/srt/layers/linear.py(168): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_0 <- sglang/srt/models/gpt2.py(84): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Attention_0 <- sglang/srt/models/gpt2.py(161): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Block_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "bia"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          1536
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          1536,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "beta"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "alpha"
      }
    ]
  },
  {
    "kernel_name": "void cutlass::Kernel2<cutlass_80_tensorop_bf16_s16816gemm_relu_bf16_64x64_64x6_tn_align8>(cutlass_80_tensorop_bf16_s16816gemm_relu_bf16_64x64_64x6_tn_align8::Params)",
    "kernel_implementation": "aten::addmm",
    "op_mapping": "matmul",
    "operation": "out = alpha*mat1 @ mat2 + beta*bias",
    "source_code": "addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor",
    "call_stack": "cuLaunchKernel <- aten::addmm <- aten::linear <- <built-in function linear> <- sglang/srt/layers/linear.py(168): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: ColumnParallelLinear_0 <- sglang/srt/models/gpt2.py(124): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2MLP_0 <- sglang/srt/models/gpt2.py(161): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Block_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "bia"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          1536
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          1536,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "beta"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "alpha"
      }
    ]
  },
  {
    "kernel_name": "void cublasLt::splitKreduce_kernel<32, 16, int, __nv_bfloat16, __nv_bfloat16, float, __nv_bfloat16, true, true, false>(cublasLt::cublasSplitKParams<float>, __nv_bfloat16 const*, __nv_bfloat16 const*, __nv_bfloat16*, float const*, float const*, __nv_bfloat16 const*, __nv_bfloat16 const*, __nv_bfloat16*, void*, long, float*, int*)",
    "kernel_implementation": "aten::addmm",
    "op_mapping": "matmul",
    "operation": "out = alpha*mat1 @ mat2 + beta*bias",
    "source_code": "addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::addmm <- aten::linear <- <built-in function linear> <- sglang/srt/layers/linear.py(168): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: ColumnParallelLinear_0 <- sglang/srt/models/gpt2.py(124): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2MLP_0 <- sglang/srt/models/gpt2.py(161): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Block_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "bia"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          1536
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          1536,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "beta"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "alpha"
      }
    ]
  },
  {
    "kernel_name": "std::enable_if<!(false), void>::type internal::gemvx::kernel<int, int, __nv_bfloat16, __nv_bfloat16, __nv_bfloat16, float, false, true, true, false, 7, false, cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<__nv_bfloat16 const>, cublasGemvTensorStridedBatched<__nv_bfloat16 const>, cublasGemvTensorStridedBatched<__nv_bfloat16>, float> >(cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<__nv_bfloat16 const>, cublasGemvTensorStridedBatched<__nv_bfloat16 const>, cublasGemvTensorStridedBatched<__nv_bfloat16>, float>)",
    "kernel_implementation": "aten::mm",
    "op_mapping": "matmul",
    "operation": "out = mat1 @ mat2",
    "source_code": "mm(Tensor self, Tensor mat2) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::mm <- aten::matmul <- aten::linear <- <built-in function linear> <- sglang/srt/layers/linear.py(168): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: QKVParallelLinear_0 <- sglang/srt/models/grok.py(221): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Attention_0 <- sglang/srt/models/grok.py(350): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1DecoderLayer_0 <- sglang/srt/models/grok.py(437): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Model_0 <- sglang/srt/models/grok.py(572): forward <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          12288,
          6288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      }
    ]
  },
  {
    "kernel_name": "void gemv2T_kernel_val<int, int, __nv_bfloat16, __nv_bfloat16, __nv_bfloat16, float, 128, 16, 4, 4, false, false, cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<__nv_bfloat16 const>, cublasGemvTensorStridedBatched<__nv_bfloat16 const>, cublasGemvTensorStridedBatched<__nv_bfloat16>, float> >(cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<__nv_bfloat16 const>, cublasGemvTensorStridedBatched<__nv_bfloat16 const>, cublasGemvTensorStridedBatched<__nv_bfloat16>, float>, float, float)",
    "kernel_implementation": "aten::mm",
    "op_mapping": "matmul",
    "operation": "out = mat1 @ mat2",
    "source_code": "mm(Tensor self, Tensor mat2) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::mm <- aten::matmul <- aten::linear <- <built-in function linear> <- sglang/srt/layers/linear.py(168): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_0 <- sglang/srt/models/grok.py(221): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Attention_0 <- sglang/srt/models/grok.py(350): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1DecoderLayer_0 <- sglang/srt/models/grok.py(437): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Grok1Model_0 <- sglang/srt/models/grok.py(572): forward <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          12288,
          6288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      }
    ]
  },
  {
    "kernel_name": "void cublasLt::splitKreduce_kernel<32, 16, int, __nv_bfloat16, __nv_bfloat16, float, __nv_bfloat16, true, false, false>(cublasLt::cublasSplitKParams<float>, __nv_bfloat16 const*, __nv_bfloat16 const*, __nv_bfloat16*, float const*, float const*, __nv_bfloat16 const*, __nv_bfloat16 const*, __nv_bfloat16*, void*, long, float*, int*)",
    "kernel_implementation": "aten::mm",
    "op_mapping": "matmul",
    "operation": "out = mat1 @ mat2",
    "source_code": "mm(Tensor self, Tensor mat2) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::mm <- aten::matmul <- aten::linear <- <built-in function linear> <- sglang/srt/layers/linear.py(168): apply <- sglang/srt/layers/linear.py(285): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: ReplicatedLinear_0 <- sglang/srt/models/qwen3_moe.py(168): forward_normal <- sglang/srt/models/qwen3_moe.py(152): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeSparseMoeBlock_0 <- sglang/srt/models/qwen3_moe.py(559): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeDecoderLayer_0 <- sglang/srt/models/qwen2_moe.py(431): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeModel_0 <- sglang/srt/models/qwen3_moe.py(690): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          12288,
          6288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<3072u, 1536u, 256u, 96u, 128u, 0u, 64u, 1u, 3u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x783ad4f31c20> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: ColumnParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(855): forward_normal_prepare <- sglang/srt/models/deepseek_v2.py(802): forward_prepare <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<4096u, 512u, 256u, 128u, 128u, 0u, 128u, 1u, 3u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x783ad4f31c20> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: ColumnParallelLinear_1 <- sglang/srt/models/deepseek_v2.py(855): forward_normal_prepare <- sglang/srt/models/deepseek_v2.py(802): forward_prepare <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 2048u, 256u, 112u, 128u, 0u, 32u, 1u, 3u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x783ad4f31c20> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(899): forward_normal_core <- sglang/srt/models/deepseek_v2.py(837): forward_core <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<4608u, 7168u, 128u, 144u, 128u, 0u, 32u, 1u, 5u, 128u, 128u, 2u, false, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x783ad4f31c20> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MergedColumnParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 2304u, 256u, 112u, 128u, 0u, 32u, 1u, 3u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x783ad4f31c20> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_1 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<512u, 7168u, 64u, 64u, 128u, 0u, 128u, 1u, 8u, 128u, 128u, 2u, false, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x783ad4f31c20> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MergedColumnParallelLinear_1 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_1 <- sglang/srt/models/deepseek_v2.py(420): _forward_shared_experts <- sglang/srt/models/deepseek_v2.py(333): forward_normal <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 256u, 256u, 112u, 128u, 0u, 32u, 1u, 2u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x783ad4f31c20> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_3 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_1 <- sglang/srt/models/deepseek_v2.py(420): _forward_shared_experts <- sglang/srt/models/deepseek_v2.py(333): forward_normal <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "sm90_xmma_gemm_bf16bf16_bf16f32_f32_tn_n_tilesize64x64x64_warpgroupsize1x1x1_execute_segment_k_on_kernel__5x_cublas",
    "kernel_implementation": "aten::mm",
    "op_mapping": "matmul",
    "operation": "out = mat1 @ mat2",
    "source_code": "mm(Tensor self, Tensor mat2) -> Tensor",
    "call_stack": "cudaLaunchKernelExC <- aten::mm <- aten::matmul <- <built-in method matmul of type object at 0x785e1b21fec0> <- sglang/srt/layers/logits_processor.py(435): _get_logits <- sglang/srt/layers/logits_processor.py(242): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: LogitsProcessor_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          12288,
          6288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<3072u, 1536u, 64u, 24u, 128u, 0u, 0u, 1u, 8u, 128u, 128u, 1u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x783ad4f31c20> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: ColumnParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(905): forward_absorb_prepare <- sglang/srt/models/deepseek_v2.py(802): forward_prepare <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 2048u, 64u, 56u, 128u, 0u, 0u, 1u, 8u, 128u, 128u, 1u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x783ad4f31c20> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(982): forward_absorb_core <- sglang/srt/models/deepseek_v2.py(837): forward_core <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<4608u, 7168u, 64u, 40u, 128u, 0u, 0u, 1u, 8u, 128u, 128u, 1u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x783ad4f31c20> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MergedColumnParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 2304u, 64u, 56u, 128u, 0u, 0u, 1u, 8u, 128u, 128u, 1u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x783ad4f31c20> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_1 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<512u, 7168u, 64u, 16u, 128u, 0u, 32u, 1u, 8u, 128u, 128u, 1u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x783ad4f31c20> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MergedColumnParallelLinear_1 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_1 <- sglang/srt/models/deepseek_v2.py(420): _forward_shared_experts <- sglang/srt/models/deepseek_v2.py(333): forward_normal <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 256u, 64u, 56u, 128u, 0u, 0u, 1u, 2u, 128u, 128u, 1u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x783ad4f31c20> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_3 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_1 <- sglang/srt/models/deepseek_v2.py(420): _forward_shared_experts <- sglang/srt/models/deepseek_v2.py(333): forward_normal <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm_bf16_64x64_64x4_tn_align8>(cutlass_80_tensorop_s16816gemm_bf16_64x64_64x4_tn_align8::Params)",
    "kernel_implementation": "aten::mm",
    "op_mapping": "matmul",
    "operation": "out = mat1 @ mat2",
    "source_code": "mm(Tensor self, Tensor mat2) -> Tensor",
    "call_stack": "cuLaunchKernel <- aten::mm <- aten::matmul <- aten::linear <- <built-in function linear> <- sglang/srt/models/deepseek_v2.py(209): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MoEGate_0 <- sglang/srt/models/deepseek_v2.py(333): forward_normal <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          12288,
          6288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      }
    ]
  },
  {
    "kernel_name": "void cublasLt::splitKreduce_kernel<32, 16, int, float, __nv_bfloat16, float, __nv_bfloat16, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, __nv_bfloat16 const*, __nv_bfloat16*, float const*, float const*, __nv_bfloat16 const*, float const*, __nv_bfloat16*, void*, long, float*, int*)",
    "kernel_implementation": "aten::mm",
    "op_mapping": "matmul",
    "operation": "out = mat1 @ mat2",
    "source_code": "mm(Tensor self, Tensor mat2) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::mm <- aten::matmul <- aten::linear <- <built-in function linear> <- sglang/srt/models/deepseek_v2.py(209): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MoEGate_0 <- sglang/srt/models/deepseek_v2.py(333): forward_normal <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          12288,
          6288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      }
    ]
  },
  {
    "kernel_name": "void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm_bf16_128x64_64x4_tn_align8>(cutlass_80_tensorop_s16816gemm_bf16_128x64_64x4_tn_align8::Params)",
    "kernel_implementation": "aten::mm",
    "op_mapping": "matmul",
    "operation": "out = mat1 @ mat2",
    "source_code": "mm(Tensor self, Tensor mat2) -> Tensor",
    "call_stack": "cuLaunchKernel <- aten::mm <- aten::matmul <- aten::linear <- <built-in function linear> <- sglang/srt/models/deepseek_v2.py(209): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MoEGate_0 <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          12288,
          6288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<4096u, 7168u, 64u, 64u, 128u, 0u, 128u, 1u, 8u, 128u, 128u, 1u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x7ad118770ed0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MergedColumnParallelLinear_1 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_1 <- sglang/srt/models/deepseek_v2.py(420): _forward_shared_experts <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 2048u, 64u, 112u, 128u, 0u, 32u, 1u, 8u, 128u, 128u, 1u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x7ad118770ed0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_3 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_1 <- sglang/srt/models/deepseek_v2.py(420): _forward_shared_experts <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "sm90_xmma_gemm_bf16bf16_bf16f32_f32_tn_n_tilesize64x128x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas",
    "kernel_implementation": "aten::mm",
    "op_mapping": "matmul",
    "operation": "out = mat1 @ mat2",
    "source_code": "mm(Tensor self, Tensor mat2) -> Tensor",
    "call_stack": "cudaLaunchKernelExC <- aten::mm <- aten::matmul <- <built-in method matmul of type object at 0x778a2541fec0> <- sglang/srt/layers/logits_processor.py(435): _get_logits <- sglang/srt/layers/logits_processor.py(242): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: LogitsProcessor_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          12288,
          6288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<1536u, 1536u, 64u, 16u, 128u, 0u, 32u, 1u, 8u, 128u, 128u, 1u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x7766dee49650> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: ColumnParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(905): forward_absorb_prepare <- sglang/srt/models/deepseek_v2.py(802): forward_prepare <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 1024u, 64u, 56u, 128u, 0u, 0u, 1u, 8u, 128u, 128u, 1u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x7766dee49650> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(982): forward_absorb_core <- sglang/srt/models/deepseek_v2.py(837): forward_core <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<2304u, 7168u, 64u, 24u, 128u, 0u, 0u, 1u, 8u, 128u, 128u, 1u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x7766dee49650> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MergedColumnParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 1152u, 64u, 56u, 128u, 0u, 0u, 1u, 8u, 128u, 128u, 1u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x7766dee49650> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_1 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<2304u, 7168u, 256u, 128u, 128u, 0u, 128u, 1u, 3u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x79a0b5c32be0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MergedColumnParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 1152u, 256u, 128u, 128u, 0u, 128u, 1u, 3u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x79a0b5c32be0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_1 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<256u, 7168u, 256u, 128u, 128u, 0u, 128u, 1u, 3u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x79a0b5c32be0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MergedColumnParallelLinear_1 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_1 <- sglang/srt/models/deepseek_v2.py(420): _forward_shared_experts <- sglang/srt/models/deepseek_v2.py(333): forward_normal <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 128u, 256u, 128u, 128u, 0u, 128u, 1u, 1u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x79a0b5c32be0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_3 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_1 <- sglang/srt/models/deepseek_v2.py(420): _forward_shared_experts <- sglang/srt/models/deepseek_v2.py(333): forward_normal <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "sm90_xmma_gemm_bf16bf16_bf16f32_f32_tn_n_tilesize256x128x64_warpgroupsize2x1x1_execute_segment_k_off_kernel__5x_cublas",
    "kernel_implementation": "aten::mm",
    "op_mapping": "matmul",
    "operation": "out = mat1 @ mat2",
    "source_code": "mm(Tensor self, Tensor mat2) -> Tensor",
    "call_stack": "cudaLaunchKernelExC <- aten::mm <- aten::matmul <- aten::linear <- <built-in function linear> <- sglang/srt/models/deepseek_v2.py(209): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MoEGate_0 <- sglang/srt/models/deepseek_v2.py(333): forward_normal <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          12288,
          6288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<256u, 7168u, 64u, 16u, 128u, 0u, 32u, 1u, 8u, 128u, 128u, 1u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x79a0b5c32be0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MergedColumnParallelLinear_1 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_1 <- sglang/srt/models/deepseek_v2.py(420): _forward_shared_experts <- sglang/srt/models/deepseek_v2.py(333): forward_normal <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 128u, 64u, 56u, 128u, 0u, 0u, 1u, 1u, 128u, 128u, 1u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x79a0b5c32be0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_3 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_1 <- sglang/srt/models/deepseek_v2.py(420): _forward_shared_experts <- sglang/srt/models/deepseek_v2.py(333): forward_normal <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<4096u, 7168u, 64u, 32u, 128u, 0u, 64u, 1u, 8u, 128u, 128u, 1u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x7ad118770ed0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MergedColumnParallelLinear_1 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_1 <- sglang/srt/models/deepseek_v2.py(420): _forward_shared_experts <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<24576u, 1536u, 256u, 128u, 128u, 0u, 128u, 1u, 3u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x730a2900adf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: ColumnParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(855): forward_normal_prepare <- sglang/srt/models/deepseek_v2.py(802): forward_prepare <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<32768u, 512u, 256u, 128u, 128u, 0u, 128u, 1u, 3u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x730a2900adf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: ColumnParallelLinear_1 <- sglang/srt/models/deepseek_v2.py(855): forward_normal_prepare <- sglang/srt/models/deepseek_v2.py(802): forward_prepare <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 16384u, 256u, 112u, 128u, 0u, 32u, 1u, 3u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x730a2900adf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(899): forward_normal_core <- sglang/srt/models/deepseek_v2.py(837): forward_core <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<4608u, 7168u, 256u, 128u, 128u, 0u, 128u, 1u, 3u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x730a2900adf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MergedColumnParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 2304u, 256u, 128u, 128u, 0u, 128u, 1u, 3u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x730a2900adf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_1 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<512u, 7168u, 256u, 128u, 128u, 0u, 128u, 1u, 3u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x730a2900adf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MergedColumnParallelLinear_1 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_1 <- sglang/srt/models/deepseek_v2.py(420): _forward_shared_experts <- sglang/srt/models/deepseek_v2.py(333): forward_normal <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 256u, 256u, 128u, 128u, 0u, 128u, 1u, 2u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x730a2900adf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_3 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_1 <- sglang/srt/models/deepseek_v2.py(420): _forward_shared_experts <- sglang/srt/models/deepseek_v2.py(333): forward_normal <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<24576u, 1536u, 64u, 96u, 128u, 0u, 64u, 1u, 4u, 128u, 128u, 1u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x730a2900adf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: ColumnParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(905): forward_absorb_prepare <- sglang/srt/models/deepseek_v2.py(802): forward_prepare <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 16384u, 64u, 56u, 128u, 0u, 0u, 1u, 8u, 128u, 128u, 1u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x730a2900adf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(982): forward_absorb_core <- sglang/srt/models/deepseek_v2.py(837): forward_core <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm_bf16_64x64_32x6_tn_align8>(cutlass_80_tensorop_s16816gemm_bf16_64x64_32x6_tn_align8::Params)",
    "kernel_implementation": "aten::mm",
    "op_mapping": "matmul",
    "operation": "out = mat1 @ mat2",
    "source_code": "mm(Tensor self, Tensor mat2) -> Tensor",
    "call_stack": "cuLaunchKernel <- aten::mm <- aten::matmul <- aten::linear <- <built-in function linear> <- sglang/srt/models/deepseek_v2.py(209): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MoEGate_0 <- sglang/srt/models/deepseek_v2.py(333): forward_normal <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          12288,
          6288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<4096u, 7168u, 256u, 128u, 128u, 0u, 128u, 1u, 3u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x721d6813edf0> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MergedColumnParallelLinear_1 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_1 <- sglang/srt/models/deepseek_v2.py(420): _forward_shared_experts <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<1536u, 1536u, 128u, 96u, 128u, 0u, 64u, 1u, 4u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x7766dee49650> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: ColumnParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(855): forward_normal_prepare <- sglang/srt/models/deepseek_v2.py(802): forward_prepare <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<2048u, 512u, 128u, 128u, 128u, 0u, 128u, 1u, 4u, 128u, 128u, 2u, false, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x7766dee49650> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: ColumnParallelLinear_1 <- sglang/srt/models/deepseek_v2.py(855): forward_normal_prepare <- sglang/srt/models/deepseek_v2.py(802): forward_prepare <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 1024u, 256u, 112u, 128u, 0u, 32u, 1u, 3u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x7766dee49650> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(899): forward_normal_core <- sglang/srt/models/deepseek_v2.py(837): forward_core <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<2304u, 7168u, 128u, 144u, 128u, 0u, 32u, 1u, 5u, 128u, 128u, 2u, false, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x7766dee49650> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(440): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MergedColumnParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 1152u, 256u, 112u, 128u, 0u, 32u, 1u, 3u, 128u, 128u, 2u, true, (deep_gemm::GemmType)0>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/gemm.py(144): gemm_fp8_fp8_bf16_nt <- sglang/srt/layers/quantization/deep_gemm.py(356): gemm_nt_f8f8bf16 <- sglang/srt/layers/quantization/fp8_kernel.py(73): deep_gemm_fp8_fp8_bf16_nt <- sglang::deep_gemm_fp8_fp8_bf16_nt <- <built-in method deep_gemm_fp8_fp8_bf16_nt of PyCapsule object at 0x7766dee49650> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/quantization/fp8_kernel.py(774): w8a8_block_fp8_matmul_deepgemm <- sglang/srt/layers/quantization/fp8_utils.py(217): deepgemm_w8a8_block_fp8_linear_with_fallback <- sglang/srt/layers/quantization/fp8.py(403): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_1 <- sglang/srt/models/deepseek_v2.py(182): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MLP_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          16
        ],
        "example_dtype": "float",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2048
        ],
        "example_dtype": "c10::Float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          56,
          16
        ],
        "example_dtype": "float",
        "description": "rhs_scale"
      },
      {
        "id": 4,
        "role": "output",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void cutlass::Kernel2<cutlass_80_tensorop_s16816gemm_bf16_64x64_64x6_tn_align8>(cutlass_80_tensorop_s16816gemm_bf16_64x64_64x6_tn_align8::Params)",
    "kernel_implementation": "aten::mm",
    "op_mapping": "matmul",
    "operation": "out = mat1 @ mat2",
    "source_code": "mm(Tensor self, Tensor mat2) -> Tensor",
    "call_stack": "cuLaunchKernel <- aten::mm <- aten::matmul <- aten::linear <- <built-in function linear> <- sglang/srt/models/deepseek_v2.py(209): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MoEGate_0 <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          12288,
          6288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      }
    ]
  },
  {
    "kernel_name": "sm90_xmma_gemm_bf16bf16_bf16f32_f32_tn_n_tilesize128x64x64_warpgroupsize1x1x1_execute_segment_k_off_kernel__5x_cublas",
    "kernel_implementation": "aten::mm",
    "op_mapping": "matmul",
    "operation": "out = mat1 @ mat2",
    "source_code": "mm(Tensor self, Tensor mat2) -> Tensor",
    "call_stack": "cudaLaunchKernelExC <- aten::mm <- aten::matmul <- aten::linear <- <built-in function linear> <- sglang/srt/models/deepseek_v2.py(209): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: MoEGate_0 <- sglang/srt/models/deepseek_v2.py(333): forward_normal <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- multiprocessing/process.py(108): run <- multiprocessing/process.py(314): _bootstrap <- multiprocessing/spawn.py(129): _main <- multiprocessing/spawn.py(116): spawn_main <- <string>(1): <module>",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          12288,
          6288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      }
    ]
  },
  {
    "kernel_name": "void cutlass::Kernel2<cutlass_80_tensorop_bf16_s16816gemm_relu_bf16_64x64_64x4_tn_align8>(cutlass_80_tensorop_bf16_s16816gemm_relu_bf16_64x64_64x4_tn_align8::Params)",
    "kernel_implementation": "aten::mm",
    "op_mapping": "matmul",
    "operation": "out = mat1 @ mat2",
    "source_code": "mm(Tensor self, Tensor mat2) -> Tensor",
    "call_stack": "cuLaunchKernel <- aten::mm <- aten::matmul <- aten::linear <- <built-in function linear> <- sglang/srt/layers/linear.py(168): apply <- sglang/srt/layers/linear.py(1277): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: RowParallelLinear_0 <- sglang/srt/models/deepseek_v2.py(982): forward_absorb_core <- sglang/srt/models/deepseek_v2.py(837): forward_core <- sglang/srt/models/deepseek_v2.py(787): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- multiprocessing/process.py(108): run <- multiprocessing/process.py(314): _bootstrap <- multiprocessing/spawn.py(129): _main <- multiprocessing/spawn.py(116): spawn_main <- <string>(1): <module>",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          12288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          12288,
          6288
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<4096u, 7168u, 128u, 160u, 128u, 0u, 64u, 32u, 4u, 128u, 128u, 2u, false, (deep_gemm::GemmType)1>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/m_grouped_gemm.py(13): m_grouped_gemm_fp8_fp8_bf16_nt_contiguous <- sglang/srt/layers/moe/ep_moe/layer.py(1075): forward_deepgemm_contiguous <- sglang/srt/layers/moe/ep_moe/layer.py(931): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepEPMoE_0 <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          7168
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1024,
          56
        ],
        "example_dtype": "torch.float32",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          16,
          4096,
          7168
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          16,
          32,
          56
        ],
        "example_dtype": "torch.float32",
        "description": "rhs_scale"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 2048u, 128u, 160u, 128u, 0u, 64u, 32u, 4u, 128u, 128u, 2u, false, (deep_gemm::GemmType)1>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/m_grouped_gemm.py(13): m_grouped_gemm_fp8_fp8_bf16_nt_contiguous <- sglang/srt/layers/moe/ep_moe/layer.py(1075): forward_deepgemm_contiguous <- sglang/srt/layers/moe/ep_moe/layer.py(931): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepEPMoE_0 <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          7168
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1024,
          56
        ],
        "example_dtype": "torch.float32",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          16,
          4096,
          7168
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          16,
          32,
          56
        ],
        "example_dtype": "torch.float32",
        "description": "rhs_scale"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<4096u, 7168u, 128u, 128u, 128u, 0u, 128u, 32u, 5u, 128u, 128u, 2u, false, (deep_gemm::GemmType)1>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/m_grouped_gemm.py(13): m_grouped_gemm_fp8_fp8_bf16_nt_contiguous <- sglang/srt/layers/moe/ep_moe/layer.py(1075): forward_deepgemm_contiguous <- sglang/srt/layers/moe/ep_moe/layer.py(931): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepEPMoE_0 <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          7168
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1024,
          56
        ],
        "example_dtype": "torch.float32",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          16,
          4096,
          7168
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          16,
          32,
          56
        ],
        "example_dtype": "torch.float32",
        "description": "rhs_scale"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<4096u, 7168u, 128u, 160u, 128u, 0u, 64u, 16u, 4u, 128u, 128u, 2u, false, (deep_gemm::GemmType)1>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/m_grouped_gemm.py(13): m_grouped_gemm_fp8_fp8_bf16_nt_contiguous <- sglang/srt/layers/moe/ep_moe/layer.py(1075): forward_deepgemm_contiguous <- sglang/srt/layers/moe/ep_moe/layer.py(931): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepEPMoE_0 <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          7168
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1024,
          56
        ],
        "example_dtype": "torch.float32",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          16,
          4096,
          7168
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          16,
          32,
          56
        ],
        "example_dtype": "torch.float32",
        "description": "rhs_scale"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<7168u, 2048u, 128u, 160u, 128u, 0u, 64u, 16u, 4u, 128u, 128u, 2u, false, (deep_gemm::GemmType)1>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/m_grouped_gemm.py(13): m_grouped_gemm_fp8_fp8_bf16_nt_contiguous <- sglang/srt/layers/moe/ep_moe/layer.py(1075): forward_deepgemm_contiguous <- sglang/srt/layers/moe/ep_moe/layer.py(931): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepEPMoE_0 <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          2048
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          8192,
          16
        ],
        "example_dtype": "torch.float32",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          16,
          7168,
          2048
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          16,
          56,
          16
        ],
        "example_dtype": "torch.float32",
        "description": "rhs_scale"
      }
    ]
  },
  {
    "kernel_name": "void deep_gemm::fp8_gemm_kernel<4096u, 7168u, 128u, 128u, 128u, 0u, 128u, 16u, 5u, 128u, 128u, 2u, false, (deep_gemm::GemmType)1>(__nv_bfloat16*, float*, int*, unsigned int, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st, CUtensorMap_st)",
    "kernel_implementation": "cuda/bindings/driver.pyx(33813): cuLaunchKernelEx",
    "op_mapping": "matmul",
    "operation": "out = (lhs, lhs_scale) @ (rhs, rhs_scale)",
    "source_code": "https://github.com/deepseek-ai/DeepGEMM/blob/03d0be3d2d03b6eed3c99d683c0620949a13a826/deep_gemm/jit_kernels/gemm.py#L153",
    "call_stack": "cuLaunchKernelEx <- cuda/bindings/driver.pyx(33813): cuLaunchKernelEx <- deep_gemm/jit_kernels/runtime.py(202): launch <- deep_gemm/jit/runtime.py(36): __call__ <- deep_gemm/jit_kernels/m_grouped_gemm.py(13): m_grouped_gemm_fp8_fp8_bf16_nt_contiguous <- sglang/srt/layers/moe/ep_moe/layer.py(1075): forward_deepgemm_contiguous <- sglang/srt/layers/moe/ep_moe/layer.py(931): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepEPMoE_0 <- sglang/srt/models/deepseek_v2.py(347): forward_deepep <- sglang/srt/models/deepseek_v2.py(325): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          7168
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "lhs"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1024,
          56
        ],
        "example_dtype": "torch.float32",
        "description": "lhs_scale"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          16,
          4096,
          7168
        ],
        "example_dtype": "torch.float8_e4m3fn",
        "description": "rhs"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          16,
          32,
          56
        ],
        "example_dtype": "torch.float32",
        "description": "rhs_scale"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<c10::BFloat16, c10::BFloat16, c10::BFloat16, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::BFloat16, c10::BFloat16, c10::BFloat16, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",
    "kernel_implementation": "aten::mul",
    "op_mapping": "mul",
    "operation": "c = a*X",
    "source_code": "sglang/srt/layers/activation.py(77)",
    "call_stack": "cudaLaunchKernel <- aten::mul <- sglang/srt/layers/activation.py(77): forward_native <- sglang/srt/layers/activation.py(81): forward_cuda <- sglang/srt/custom_op.py(34): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: NewGELU_0 <- sglang/srt/models/gpt2.py(124): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2MLP_0 <- sglang/srt/models/gpt2.py(161): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Block_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          1536
        ],
        "example_dtype": "c10::BFloat16",
        "description": "X"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "double",
        "description": "a"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<c10::BFloat16, c10::BFloat16, c10::BFloat16, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul> >(int, at::native::BinaryFunctor<c10::BFloat16, c10::BFloat16, c10::BFloat16, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 3ul>)",
    "kernel_implementation": "aten::mul",
    "op_mapping": "mul",
    "operation": "c = a*X",
    "source_code": "sglang/srt/layers/activation.py(77)",
    "call_stack": "cudaLaunchKernel <- aten::mul <- sglang/srt/layers/activation.py(77): forward_native <- sglang/srt/layers/activation.py(81): forward_cuda <- sglang/srt/custom_op.py(34): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: NewGELU_0 <- sglang/srt/models/gpt2.py(124): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2MLP_0 <- sglang/srt/models/gpt2.py(161): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Block_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          1536
        ],
        "example_dtype": "c10::BFloat16",
        "description": "X"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1024,
          1536
        ],
        "example_dtype": "c10::BFloat16",
        "description": "A"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl<c10::BFloat16, c10::BFloat16>(at::TensorIteratorBase&, c10::BFloat16)::{lambda(c10::BFloat16)#2}, std::array<char*, 2ul> >(int, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl<c10::BFloat16, c10::BFloat16>(at::TensorIteratorBase&, c10::BFloat16)::{lambda(c10::BFloat16)#2}, std::array<char*, 2ul>)",
    "kernel_implementation": "aten::pow",
    "op_mapping": "pow",
    "operation": "torch.pow(X, a)",
    "source_code": "sglang/srt/layers/activation.py(77)",
    "call_stack": "cudaLaunchKernel <- aten::pow <- <built-in method pow of type object at 0x7352e621fec0> <- sglang/srt/layers/activation.py(77): forward_native <- sglang/srt/layers/activation.py(81): forward_cuda <- sglang/srt/custom_op.py(34): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: NewGELU_0 <- sglang/srt/models/gpt2.py(124): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2MLP_0 <- sglang/srt/models/gpt2.py(161): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Block_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          1536
        ],
        "example_dtype": "c10::BFloat16",
        "description": "X"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "a"
      }
    ]
  },
  {
    "kernel_name": "ncclDevKernel_ReduceScatter_Sum_bf16_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",
    "kernel_implementation": "nccl:reduce_scatter",
    "op_mapping": "reduce_scatter",
    "operation": "out = reduce_scatter(rank_i_input)",
    "source_code": "",
    "call_stack": "cudaLaunchKernelExC <- nccl:reduce_scatter <- record_param_comms <- c10d::reduce_scatter_ <- torch/distributed/distributed_c10d.py(4118): reduce_scatter <- torch/distributed/c10d_logger.py(78): wrapper <- sglang/srt/distributed/parallel_state.py(463): reduce_scatter <- sglang/srt/layers/dp_attention.py(295): attn_tp_reduce_scatter <- sglang/srt/layers/communicator.py(386): _scatter_hidden_states_and_residual <- sglang/srt/layers/communicator.py(199): prepare_mlp <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- multiprocessing/process.py(108): run <- multiprocessing/process.py(314): _bootstrap <- multiprocessing/spawn.py(129): _main <- multiprocessing/spawn.py(116): spawn_main <- <string>(1): <module>",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          128,
          7168
        ],
        "example_dtype": "torch.bfloat16",
        "description": "rank i input"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "int",
        "description": "TP"
      }
    ]
  },
  {
    "kernel_name": "ncclDevKernel_Reduce_Sum_bf16_RING_LL(ncclDevComm*, unsigned long, ncclWork*)",
    "kernel_implementation": "record_param_comms",
    "op_mapping": "reduce_scatter",
    "operation": "out = reduce_scatter(rank_i_input)",
    "source_code": "",
    "call_stack": "cudaLaunchKernelExC <- record_param_comms <- c10d::reduce_scatter_ <- torch/distributed/distributed_c10d.py(4118): reduce_scatter <- torch/distributed/c10d_logger.py(78): wrapper <- sglang/srt/distributed/parallel_state.py(463): reduce_scatter <- sglang/srt/layers/dp_attention.py(295): attn_tp_reduce_scatter <- sglang/srt/layers/communicator.py(386): _scatter_hidden_states_and_residual <- sglang/srt/layers/communicator.py(199): prepare_mlp <- sglang/srt/models/deepseek_v2.py(1442): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(1583): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(1716): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1139): forward_decode <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(255): decode <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- multiprocessing/process.py(108): run <- multiprocessing/process.py(314): _bootstrap <- multiprocessing/spawn.py(129): _main <- multiprocessing/spawn.py(116): spawn_main <- <string>(1): <module>",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "torch.bfloat16",
        "description": ""
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "int",
        "description": "TP"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<long>, std::array<char*, 3ul> >(int, at::native::CUDAFunctor_add<long>, std::array<char*, 3ul>)",
    "kernel_implementation": "aten::sub",
    "op_mapping": "sub",
    "operation": "out = self - alpha * other",
    "source_code": "sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::sub <- sglang/srt/managers/schedule_batch.py(1117): prepare_for_extend <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "long int",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "long int",
        "description": "other"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "alpha"
      }
    ]
  },
  {
    "kernel_name": "void at::native::unrolled_elementwise_kernel<at::native::CUDAFunctor_add<long>, std::array<char*, 3ul>, 4, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithCast<2>, at::native::memory::StoreWithCast<1> >(int, at::native::CUDAFunctor_add<long>, std::array<char*, 3ul>, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithCast<2>, at::native::memory::StoreWithCast<1>)",
    "kernel_implementation": "aten::sub",
    "op_mapping": "sub",
    "operation": "out = self - alpha * other",
    "source_code": "sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::sub <- sglang/srt/layers/attention/flashinfer_backend.py(898): call_begin_forward <- sglang/srt/layers/attention/flashinfer_backend.py(786): update_single_wrapper <- sglang/srt/layers/attention/flashinfer_backend.py(197): init_forward_metadata <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "long int",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "int",
        "description": "other"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "alpha"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, std::array<char*, 2ul> >(int, at::native::CUDAFunctorOnSelf_add<long>, std::array<char*, 2ul>)",
    "kernel_implementation": "aten::sub",
    "op_mapping": "sub",
    "operation": "out = self - alpha * other",
    "source_code": "sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::sub <- sglang/srt/layers/logits_processor.py(242): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: LogitsProcessor_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "long int",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "long int",
        "description": "other"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "alpha"
      }
    ]
  },
  {
    "kernel_name": "void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)",
    "kernel_implementation": "aten::sum",
    "op_mapping": "sum",
    "operation": "out = sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None)",
    "source_code": "sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::sum <- aten::sum <- <built-in method sum of Tensor object at 0x73302e12a4d0> <- sglang/srt/layers/attention/flashinfer_backend.py(786): update_single_wrapper <- sglang/srt/layers/attention/flashinfer_backend.py(197): init_forward_metadata <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1
        ],
        "example_dtype": "int",
        "description": "self"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "ScalarList",
        "description": "dim"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "keepdim"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "dtype"
      }
    ]
  },
  {
    "kernel_name": "void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",
    "kernel_implementation": "aten::sum",
    "op_mapping": "sum",
    "operation": "out = sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None)",
    "source_code": "sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::sum <- <built-in method sum of Tensor object at 0x743b8e1ba980> <- sglang/srt/layers/moe/topk.py(64): fused_topk <- sglang/srt/layers/moe/topk.py(298): select_experts <- sglang/srt/layers/moe/fused_moe_triton/layer.py(156): forward_cuda <- sglang/srt/custom_op.py(34): forward <- sglang/srt/layers/moe/fused_moe_triton/layer.py(120): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(641): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/qwen3_moe.py(168): forward_normal <- sglang/srt/models/qwen3_moe.py(152): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeSparseMoeBlock_0 <- sglang/srt/models/qwen3_moe.py(559): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeDecoderLayer_0 <- sglang/srt/models/qwen2_moe.py(431): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeModel_0 <- sglang/srt/models/qwen3_moe.py(690): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "input",
        "example_dim": [
          1024,
          8
        ],
        "example_dtype": "float",
        "description": "self"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "ScalarList",
        "description": "dim"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "keepdim"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "dtype"
      }
    ]
  },
  {
    "kernel_name": "void at::native::reduce_kernel<128, 4, at::native::ReduceOp<c10::BFloat16, at::native::func_wrapper_t<c10::BFloat16, at::native::sum_functor<c10::BFloat16, float, c10::BFloat16>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::BFloat16, 4> >(at::native::ReduceOp<c10::BFloat16, at::native::func_wrapper_t<c10::BFloat16, at::native::sum_functor<c10::BFloat16, float, c10::BFloat16>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, c10::BFloat16, 4>)",
    "kernel_implementation": "aten::sum",
    "op_mapping": "sum",
    "operation": "out = sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None)",
    "source_code": "sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor",
    "call_stack": "cudaLaunchKernel <- aten::sum <- <built-in method sum of type object at 0x743fb841fec0> <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1308): fused_experts_impl <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1079): inplace_fused_experts <- sglang::inplace_fused_experts <- <built-in method inplace_fused_experts of PyCapsule object at 0x743bd049c600> <- torch/_ops.py(1112): __call__ <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1236): fused_experts <- sglang/srt/layers/moe/fused_moe_triton/layer.py(156): forward_cuda <- sglang/srt/custom_op.py(34): forward <- sglang/srt/layers/moe/fused_moe_triton/layer.py(120): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(641): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/qwen3_moe.py(168): forward_normal <- sglang/srt/models/qwen3_moe.py(152): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeSparseMoeBlock_0 <- sglang/srt/models/qwen3_moe.py(559): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeDecoderLayer_0 <- sglang/srt/models/qwen2_moe.py(431): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: Qwen3MoeModel_0 <- sglang/srt/models/qwen3_moe.py(690): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "role": "input",
        "example_dim": [
          1024,
          8,
          4096
        ],
        "example_dtype": "c10::BFloat16",
        "description": "self"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "ScalarList",
        "description": "dim"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "keepdim"
      },
      {
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "dtype"
      },
      {
        "role": "output",
        "example_dim": [
          1024,
          4096
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<4, at::native::tanh_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda(c10::BFloat16)#1}, std::array<char*, 2ul> >(int, at::native::tanh_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda(c10::BFloat16)#1}, std::array<char*, 2ul>)",
    "kernel_implementation": "aten::tanh",
    "op_mapping": "tanh",
    "operation": "C = tanh(X)",
    "source_code": "",
    "call_stack": "cudaLaunchKernel <- aten::tanh <- <built-in method tanh of type object at 0x7352e621fec0> <- sglang/srt/layers/activation.py(77): forward_native <- sglang/srt/layers/activation.py(81): forward_cuda <- sglang/srt/custom_op.py(34): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: NewGELU_0 <- sglang/srt/models/gpt2.py(124): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2MLP_0 <- sglang/srt/models/gpt2.py(161): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Block_0 <- sglang/srt/models/gpt2.py(213): forward <- torch/nn/modules/module.py(1743): _call_impl <- nn.Module: GPT2Model_0 <- sglang/srt/models/gpt2.py(249): forward <- sglang/srt/model_executor/model_runner.py(1151): forward_extend <- sglang/srt/model_executor/model_runner.py(1208): _forward_raw <- sglang/srt/model_executor/model_runner.py(1187): forward <- sglang/bench_one_batch.py(233): extend <- torch/utils/_contextlib.py(113): decorate_context <- sglang/bench_one_batch.py(389): latency_test_run_once <- sglang/bench_one_batch.py(499): latency_test <- sglang/bench_one_batch.py(548): main <- sglang/bench_one_batch.py(589): <module> <- runpy.py(86): _run_code <- runpy.py(196): _run_module_as_main",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          1536
        ],
        "example_dtype": "c10::BFloat16",
        "description": "X"
      }
    ]
  },
  {
    "kernel_name": "triton_poi_fused_index_lt_where_0",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "TBD",
    "source_code": "TBD",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(522): __call__ <- <string>(1): launcher <- triton_poi_fused_index_lt_where_0 <- torch/_inductor/runtime/triton_heuristics.py(888): run <- /tmp/torchinductor_root/4x/c4x3uy4xooqzq3n7un4l72sqajwn2ydk5crhvj3zj3rqoojno4xc.py(88): call <- torch/_inductor/utils.py(2402): run <- torch/_inductor/output_code.py(457): __call__ <- torch/_functorch/_aot_autograd/runtime_wrappers.py(481): wrapper <- torch/_functorch/_aot_autograd/utils.py(116): call_func_at_runtime_with_args <- torch/_functorch/_aot_autograd/runtime_wrappers.py(291): runtime_wrapper <- torch/_functorch/aot_autograd.py(1205): forward <- torch/_dynamo/eval_frame.py(829): _fn <- sglang/srt/managers/tp_worker_overlap_thread.py(44): resolve_future_token_ids <- Torch-Compiled Region: 0/0 <- torch/_dynamo/eval_frame.py(619): _fn <- sglang/srt/managers/tp_worker_overlap_thread.py(152): forward_thread_func_ <- torch/utils/_contextlib.py(116): decorate_context <- sglang/srt/managers/tp_worker_overlap_thread.py(140): forward_thread_func <- threading.py(953): run <- threading.py(1016): _bootstrap_inner <- threading.py(973): _bootstrap",
    "params": [
      {
        "id": 0,
        "role": "unknown",
        "example_dim": "N/A",
        "example_dtype": "N/A",
        "description": ""
      }
    ]
  },
  {
    "kernel_name": "_moe_sum_reduce_kernel",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "TBD",
    "source_code": "sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1267): moe_sum_reduce_triton",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(522): __call__ <- triton/runtime/jit.py(525): run <- triton/runtime/jit.py(347): <lambda> <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1267): moe_sum_reduce_triton <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1309): fused_experts_impl <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(978): inplace_fused_experts <- sglang::inplace_fused_experts <- <built-in method inplace_fused_experts of PyCapsule object at 0x7ca59e33d110> <- torch/_ops.py(1147): __call__ <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1142): fused_experts <- sglang/srt/layers/quantization/unquant.py(226): forward_cuda <- sglang/srt/custom_op.py(58): forward <- sglang/srt/layers/quantization/unquant.py(194): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(574): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/deepseek_v2.py(492): forward_normal <- sglang/srt/models/deepseek_v2.py(446): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1880): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(2030): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(2159): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1537): forward_extend <- sglang/srt/model_executor/model_runner.py(1621): _forward_raw <- sglang/srt/model_executor/model_runner.py(1594): forward <- sglang/srt/managers/tp_worker.py(215): forward_batch_generation <- sglang/srt/managers/tp_worker_overlap_thread.py(152): forward_thread_func_ <- torch/utils/_contextlib.py(116): decorate_context <- sglang/srt/managers/tp_worker_overlap_thread.py(140): forward_thread_func <- threading.py(953): run <- threading.py(1016): _bootstrap_inner <- threading.py(973): _bootstrap",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1024,
          6144
        ],
        "example_dtype": "c10::BFloat16",
        "description": "hidden_states"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          8,
          8192,
          6144
        ],
        "example_dtype": "c10::BFloat16",
        "description": "w1"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          8,
          6144,
          4096
        ],
        "example_dtype": "c10::BFloat16",
        "description": "w2"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          1024,
          2
        ],
        "example_dtype": "float",
        "description": "topk_weights"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [
          1024,
          2
        ],
        "example_dtype": "int",
        "description": "topk_ids"
      },
      {
        "id": 5,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "activation"
      },
      {
        "id": 6,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "apply_router_weight_on_input"
      },
      {
        "id": 7,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "use_fp8_w8a8"
      },
      {
        "id": 8,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "use_int8_w8a8"
      },
      {
        "id": 9,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "use_int8_w8a16"
      },
      {
        "id": 10,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "use_int4_w4a16"
      },
      {
        "id": 11,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "per_channel_quant"
      },
      {
        "id": 12,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "w1_scale"
      },
      {
        "id": 13,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "w2_scale"
      },
      {
        "id": 14,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "w1_zp"
      },
      {
        "id": 15,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "w2_zp"
      },
      {
        "id": 16,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "a1_scale"
      },
      {
        "id": 17,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "a2_scale"
      },
      {
        "id": 18,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "block_shape"
      }
    ]
  },
  {
    "kernel_name": "ncclDevKernel_AllGather_RING_LL(ncclDevKernelArgsStorage<4096ul>)",
    "kernel_implementation": "nccl:_all_gather_base",
    "op_mapping": "all_gather",
    "operation": "out = all_gather(rank_i_tensor)",
    "source_code": "",
    "call_stack": "cuLaunchKernelEx <- nccl:_all_gather_base <- record_param_comms <- c10d::_allgather_base_ <- torch/distributed/distributed_c10d.py(3736): all_gather_into_tensor <- torch/distributed/c10d_logger.py(78): wrapper <- sglang/srt/distributed/parallel_state.py(566): _all_gather_into_tensor <- sglang/srt/distributed/parallel_state.py(151): reg_all_gather_into_tensor <- sglang::reg_all_gather_into_tensor <- <built-in method reg_all_gather_into_tensor of PyCapsule object at 0x7ca59e211950> <- torch/_ops.py(1147): __call__ <- sglang/srt/distributed/parallel_state.py(575): all_gather_into_tensor <- sglang/srt/distributed/parallel_state.py(583): all_gather <- sglang/srt/distributed/communication_op.py(16): tensor_model_parallel_all_gather <- sglang/srt/layers/logits_processor.py(428): _get_logits <- sglang/srt/layers/logits_processor.py(235): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: LogitsProcessor_0 <- sglang/srt/models/deepseek_v2.py(2159): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1537): forward_extend <- sglang/srt/model_executor/model_runner.py(1621): _forward_raw <- sglang/srt/model_executor/model_runner.py(1594): forward <- sglang/srt/managers/tp_worker.py(215): forward_batch_generation <- sglang/srt/managers/tp_worker_overlap_thread.py(152): forward_thread_func_ <- torch/utils/_contextlib.py(116): decorate_context <- sglang/srt/managers/tp_worker_overlap_thread.py(140): forward_thread_func <- threading.py(953): run <- threading.py(1016): _bootstrap_inner <- threading.py(973): _bootstrap",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          32320
        ],
        "example_dtype": "c10::BFloat16",
        "description": "rank0 tensor to gather"
      },
      {
        "id": 1,
        "role": "N/A",
        "example_dim": [],
        "example_dtype": "",
        "description": "unrelated"
      },
      {
        "id": 2,
        "role": "N/A",
        "example_dim": [],
        "example_dtype": "",
        "description": "unrelated"
      },
      {
        "id": 3,
        "role": "N/A",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "unrelated"
      },
      {
        "id": 4,
        "role": "N/A",
        "example_dim": [],
        "example_dtype": "",
        "description": "unrelated"
      },
      {
        "id": 5,
        "role": "N/A",
        "example_dim": [],
        "example_dtype": "ScalarList",
        "description": "unrelated"
      },
      {
        "id": 6,
        "role": "N/A",
        "example_dim": [],
        "example_dtype": "ScalarList",
        "description": "unrelated"
      },
      {
        "id": 7,
        "role": "N/A",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "unrelated"
      },
      {
        "id": 8,
        "role": "N/A",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "unrelated"
      },
      {
        "id": 9,
        "role": "N/A",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "unrelated"
      }
    ]
  },
  {
    "kernel_name": "triton_per_fused_mul_sum_0",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "TBD",
    "source_code": "sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1303): moe_sum_reduce_torch_compile",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(522): __call__ <- <string>(1): launcher <- triton_per_fused_mul_sum_0 <- torch/_inductor/runtime/triton_heuristics.py(888): run <- /tmp/torchinductor_root/nz/cnz6j5fuwfvoksoqvgjueywhk7z7w76b5smlvluhhwgt5mvqnctt.py(92): call <- torch/_inductor/utils.py(2402): run <- torch/_inductor/output_code.py(457): __call__ <- torch/_functorch/_aot_autograd/runtime_wrappers.py(481): wrapper <- torch/_functorch/_aot_autograd/utils.py(116): call_func_at_runtime_with_args <- torch/_functorch/_aot_autograd/runtime_wrappers.py(291): runtime_wrapper <- torch/_functorch/aot_autograd.py(1205): forward <- torch/_dynamo/eval_frame.py(829): _fn <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1303): moe_sum_reduce_torch_compile <- Torch-Compiled Region: 1/2 <- torch/_dynamo/eval_frame.py(619): _fn <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1309): fused_experts_impl <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(978): inplace_fused_experts <- sglang::inplace_fused_experts <- <built-in method inplace_fused_experts of PyCapsule object at 0x7ca59e33d110> <- torch/_ops.py(1147): __call__ <- sglang/srt/layers/moe/fused_moe_triton/fused_moe.py(1142): fused_experts <- sglang/srt/layers/quantization/unquant.py(226): forward_cuda <- sglang/srt/custom_op.py(58): forward <- sglang/srt/layers/quantization/unquant.py(194): apply <- sglang/srt/layers/moe/fused_moe_triton/layer.py(574): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: FusedMoE_0 <- sglang/srt/models/deepseek_v2.py(492): forward_normal <- sglang/srt/models/deepseek_v2.py(446): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1880): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(2030): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(2159): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1518): forward_decode <- sglang/srt/model_executor/model_runner.py(1621): _forward_raw <- sglang/srt/model_executor/model_runner.py(1594): forward <- sglang/srt/managers/tp_worker.py(215): forward_batch_generation <- sglang/srt/managers/tp_worker_overlap_thread.py(152): forward_thread_func_ <- torch/utils/_contextlib.py(116): decorate_context <- sglang/srt/managers/tp_worker_overlap_thread.py(140): forward_thread_func <- threading.py(953): run <- threading.py(1016): _bootstrap_inner <- threading.py(973): _bootstrap",
    "params": [
      {
        "id": 0,
        "role": "N/A",
        "example_dim": "N/A",
        "example_dtype": "N/A",
        "description": "unrelated"
      }
    ]
  },
  {
    "kernel_name": "flash::prepare_varlen_num_blocks_kernel(int, int, int, int const*, int const*, int const*, int const*, int const*, int const*, int, int, int, int, int, cutlass::FastDivmod, cutlass::FastDivmod, int*, int*, bool)",
    "kernel_implementation": "sgl_kernel::fwd",
    "op_mapping": "TBD",
    "operation": "o = softmax(q@k^T)@v",
    "source_code": "sgl_kernel/flash_attn.py(31): flash_attn_with_kvcache",
    "call_stack": "cudaLaunchKernel <- sgl_kernel::fwd <- <built-in method  of PyCapsule object at 0x7f8cb4da3b70> <- torch/_ops.py(755): __call__ <- sgl_kernel/flash_attn.py(31): flash_attn_with_kvcache <- sglang/srt/layers/attention/flashattention_backend.py(621): forward_extend <- sglang/srt/layers/attention/base_attn_backend.py(57): forward <- sglang/srt/layers/radix_attention.py(84): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: RadixAttention_0 <- sglang/srt/models/qwen3_moe.py(420): forward_core <- sglang/srt/models/qwen3_moe.py(428): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: Qwen3MoeAttention_0 <- sglang/srt/models/qwen3_moe.py(521): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: Qwen3MoeDecoderLayer_0 <- sglang/srt/models/qwen2_moe.py(447): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: Qwen3MoeModel_0 <- sglang/srt/models/qwen3_moe.py(656): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1537): forward_extend <- sglang/srt/model_executor/model_runner.py(1621): _forward_raw <- sglang/srt/model_executor/model_runner.py(1594): forward <- sglang/srt/managers/tp_worker.py(215): forward_batch_generation <- sglang/srt/managers/tp_worker_overlap_thread.py(152): forward_thread_func_ <- torch/utils/_contextlib.py(116): decorate_context <- sglang/srt/managers/tp_worker_overlap_thread.py(140): forward_thread_func <- threading.py(953): run <- threading.py(1016): _bootstrap_inner <- threading.py(973): _bootstrap",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          6145,
          8,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "q"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          142748737,
          1,
          1,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "k_cache"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          142748737,
          1,
          1,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "v_cache"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "k"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "v"
      },
      {
        "id": 5,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "qv"
      },
      {
        "id": 6,
        "role": "output",
        "example_dim": [],
        "example_dtype": "",
        "description": "None - out"
      },
      {
        "id": 7,
        "role": "input",
        "example_dim": [
          5
        ],
        "example_dtype": "int",
        "description": "cu_seqlens_q"
      },
      {
        "id": 8,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "None - cu_seqlens_k"
      },
      {
        "id": 9,
        "role": "unknown",
        "example_dim": [
          5
        ],
        "example_dtype": "int",
        "description": "cu_seqlens_k_new"
      },
      {
        "id": 10,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "None - seqused_q"
      },
      {
        "id": 11,
        "role": "input",
        "example_dim": [
          4
        ],
        "example_dtype": "int",
        "description": "cache_seqlens"
      },
      {
        "id": 12,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "max_seqlen_q"
      },
      {
        "id": 13,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "None - max_seqlen_k"
      },
      {
        "id": 14,
        "role": "input",
        "example_dim": [
          4,
          2048
        ],
        "example_dtype": "int",
        "description": "page_table"
      },
      {
        "id": 15,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "cache_batch_idx"
      },
      {
        "id": 16,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "cache_leftpad"
      },
      {
        "id": 17,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "rotary_cos"
      },
      {
        "id": 18,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "rotary_sin"
      },
      {
        "id": 19,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "rotary_seqlens"
      },
      {
        "id": 20,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "q_descale"
      },
      {
        "id": 21,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "k_descale"
      },
      {
        "id": 22,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "v_descale"
      },
      {
        "id": 23,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "softmax_scale"
      },
      {
        "id": 24,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "causal"
      },
      {
        "id": 25,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "window_size[0]"
      },
      {
        "id": 26,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "window_size[1]"
      },
      {
        "id": 27,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "softcap"
      },
      {
        "id": 28,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "rotary_interleaved"
      },
      {
        "id": 29,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "scheduler_metadata"
      },
      {
        "id": 30,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "num_splits"
      },
      {
        "id": 31,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "pack_gqa"
      },
      {
        "id": 32,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "sm_margin"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<8, at::native::AUnaryFunctor<c10::BFloat16, c10::BFloat16, c10::BFloat16, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul> >(int, at::native::AUnaryFunctor<c10::BFloat16, c10::BFloat16, c10::BFloat16, at::native::binary_internal::MulFunctor<float> >, std::array<char*, 2ul>)",
    "kernel_implementation": "aten::mul_",
    "op_mapping": "mul",
    "operation": "c = a*X",
    "source_code": "aten.mul.Scalar",
    "call_stack": "cudaLaunchKernel <- aten::mul_ <- <built-in method mul_ of Tensor object at 0x7f01ea973d30> <- sglang/srt/models/grok.py(443): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: Grok1Model_0 <- sglang/srt/models/grok.py(578): forward <- sglang/srt/model_executor/model_runner.py(1537): forward_extend <- sglang/srt/model_executor/model_runner.py(1621): _forward_raw <- sglang/srt/model_executor/model_runner.py(1594): forward <- sglang/srt/managers/tp_worker.py(215): forward_batch_generation <- sglang/srt/managers/tp_worker_overlap_thread.py(152): forward_thread_func_ <- torch/utils/_contextlib.py(116): decorate_context <- sglang/srt/managers/tp_worker_overlap_thread.py(140): forward_thread_func <- threading.py(953): run <- threading.py(1016): _bootstrap_inner <- threading.py(973): _bootstrap",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          33,
          6144
        ],
        "example_dtype": "c10::BFloat16",
        "description": "X"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "double",
        "description": "a"
      }
    ]
  },
  {
    "kernel_name": "fused_moe_router_large_bs_kernel",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "Need to add more details. This is triton kernel.",
    "source_code": "sglang/srt/layers/moe/router.py(268): fused_moe_router_large_bs_impl",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(522): __call__ <- triton/runtime/jit.py(525): run <- triton/runtime/jit.py(347): <lambda> <- sglang/srt/layers/moe/router.py(268): fused_moe_router_large_bs_impl <- sglang/srt/layers/moe/router.py(311): fused_moe_router_shim <- sglang/srt/layers/moe/topk.py(658): select_experts <- sglang/srt/layers/moe/topk.py(179): forward_cuda <- sglang/srt/custom_op.py(58): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: TopK_0 <- sglang/srt/models/grok.py(140): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: Grok1MoE_0 <- sglang/srt/models/grok.py(356): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: Grok1DecoderLayer_0 <- sglang/srt/models/grok.py(443): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: Grok1Model_0 <- sglang/srt/models/grok.py(578): forward <- sglang/srt/model_executor/model_runner.py(1537): forward_extend <- sglang/srt/model_executor/model_runner.py(1621): _forward_raw <- sglang/srt/model_executor/model_runner.py(1594): forward <- sglang/srt/managers/tp_worker.py(215): forward_batch_generation <- sglang/srt/managers/tp_worker_overlap_thread.py(152): forward_thread_func_ <- torch/utils/_contextlib.py(116): decorate_context <- sglang/srt/managers/tp_worker_overlap_thread.py(140): forward_thread_func <- threading.py(953): run <- threading.py(1016): _bootstrap_inner <- threading.py(973): _bootstrap",
    "params": [
      {
        "id": 0,
        "role": "unknown",
        "example_dim": "N/A",
        "example_dtype": "N/A",
        "description": ""
      }
    ]
  },
  {
    "kernel_name": "void fused_a_gemm_kernel<1, 2112, 7168, 16, 16, 256, 12>(__nv_bfloat16*, __nv_bfloat16 const*, __nv_bfloat16 const*, int)",
    "kernel_implementation": "sgl_kernel::dsv3_fused_a_gemm",
    "op_mapping": "matmul",
    "operation": "out = mat1 @ mat2",
    "source_code": "sgl_kernel/gemm.py(85): dsv3_fused_a_gemm",
    "call_stack": "cudaLaunchKernelExC <- sgl_kernel::dsv3_fused_a_gemm <- <built-in method  of PyCapsule object at 0x7f977861ae80> <- torch/_ops.py(755): __call__ <- sgl_kernel/gemm.py(85): dsv3_fused_a_gemm <- sglang/srt/models/deepseek_v2.py(1204): forward_absorb_prepare <- sglang/srt/models/deepseek_v2.py(1092): forward_prepare <- sglang/srt/models/deepseek_v2.py(1077): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1880): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(2030): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(2159): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1518): forward_decode <- sglang/srt/model_executor/model_runner.py(1621): _forward_raw <- sglang/srt/model_executor/model_runner.py(1594): forward <- sglang/srt/managers/tp_worker.py(215): forward_batch_generation <- sglang/srt/managers/tp_worker_overlap_thread.py(152): forward_thread_func_ <- torch/utils/_contextlib.py(116): decorate_context <- sglang/srt/managers/tp_worker_overlap_thread.py(140): forward_thread_func <- threading.py(953): run <- threading.py(1016): _bootstrap_inner <- threading.py(973): _bootstrap",
    "params": [
      {
        "id": 0,
        "role": "output",
        "example_dim": [
          16,
          2112
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          16,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          7168,
          2112
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<4, at::native::compare_scalar_kernel<long>(at::TensorIteratorBase&, at::native::(anonymous namespace)::OpType, long)::{lambda(long)#1}, std::array<char*, 2ul> >(int, at::native::compare_scalar_kernel<long>(at::TensorIteratorBase&, at::native::(anonymous namespace)::OpType, long)::{lambda(long)#1}, std::array<char*, 2ul>)",
    "kernel_implementation": "aten::ge",
    "op_mapping": "TBD",
    "operation": "out[i] = (input[i] >= other[i]) ? True : False",
    "source_code": "",
    "call_stack": "cudaLaunchKernel <- aten::ge <- fallback_function <- fallback_function <- get_masked_input_and_mask <- sglang/srt/layers/vocab_parallel_embedding.py(453): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: VocabParallelEmbedding_0 <- sglang/srt/models/qwen2_moe.py(447): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: Qwen3MoeModel_0 <- sglang/srt/models/qwen3_moe.py(656): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1537): forward_extend <- sglang/srt/model_executor/model_runner.py(1621): _forward_raw <- sglang/srt/model_executor/model_runner.py(1594): forward <- sglang/srt/managers/tp_worker.py(215): forward_batch_generation <- sglang/srt/managers/tp_worker_overlap_thread.py(152): forward_thread_func_ <- torch/utils/_contextlib.py(116): decorate_context <- sglang/srt/managers/tp_worker_overlap_thread.py(140): forward_thread_func <- threading.py(953): run <- threading.py(1016): _bootstrap_inner <- threading.py(973): _bootstrap",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          2
        ],
        "example_dtype": "long int",
        "description": "input"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "other"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseAndFunctor<bool> >, std::array<char*, 3ul> >(int, at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseAndFunctor<bool> >, std::array<char*, 3ul>)",
    "kernel_implementation": "aten::bitwise_and",
    "op_mapping": "TBD",
    "operation": "out[i] = input[i] & other[i]",
    "source_code": "",
    "call_stack": "cudaLaunchKernel <- aten::bitwise_and <- aten::__and__ <- fallback_function <- fallback_function <- get_masked_input_and_mask <- sglang/srt/layers/vocab_parallel_embedding.py(453): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: VocabParallelEmbedding_0 <- sglang/srt/models/qwen2_moe.py(447): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: Qwen3MoeModel_0 <- sglang/srt/models/qwen3_moe.py(656): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1537): forward_extend <- sglang/srt/model_executor/model_runner.py(1621): _forward_raw <- sglang/srt/model_executor/model_runner.py(1594): forward <- sglang/srt/managers/tp_worker.py(215): forward_batch_generation <- sglang/srt/managers/tp_worker_overlap_thread.py(152): forward_thread_func_ <- torch/utils/_contextlib.py(116): decorate_context <- sglang/srt/managers/tp_worker_overlap_thread.py(140): forward_thread_func <- threading.py(953): run <- threading.py(1016): _bootstrap_inner <- threading.py(973): _bootstrap",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          2
        ],
        "example_dtype": "bool",
        "description": "input"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          2
        ],
        "example_dtype": "bool",
        "description": "other"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseOrFunctor<bool> >, std::array<char*, 3ul> >(int, at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseOrFunctor<bool> >, std::array<char*, 3ul>)",
    "kernel_implementation": "aten::bitwise_or",
    "op_mapping": "TBD",
    "operation": "out[i] = input[i] | other[i]",
    "source_code": "",
    "call_stack": "cudaLaunchKernel <- aten::bitwise_or <- aten::__or__ <- fallback_function <- fallback_function <- get_masked_input_and_mask <- sglang/srt/layers/vocab_parallel_embedding.py(453): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: VocabParallelEmbedding_0 <- sglang/srt/models/qwen2_moe.py(447): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: Qwen3MoeModel_0 <- sglang/srt/models/qwen3_moe.py(656): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1537): forward_extend <- sglang/srt/model_executor/model_runner.py(1621): _forward_raw <- sglang/srt/model_executor/model_runner.py(1594): forward <- sglang/srt/managers/tp_worker.py(215): forward_batch_generation <- sglang/srt/managers/tp_worker_overlap_thread.py(152): forward_thread_func_ <- torch/utils/_contextlib.py(116): decorate_context <- sglang/srt/managers/tp_worker_overlap_thread.py(140): forward_thread_func <- threading.py(953): run <- threading.py(1016): _bootstrap_inner <- threading.py(973): _bootstrap",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          2
        ],
        "example_dtype": "bool",
        "description": "input"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          2
        ],
        "example_dtype": "bool",
        "description": "other"
      }
    ]
  },
  {
    "kernel_name": "void router_gemm_kernel_bf16_output<__nv_bfloat16, 128, 8, 1, 256, 7168>(__nv_bfloat16*, __nv_bfloat16 const*, __nv_bfloat16 const*)",
    "kernel_implementation": "sgl_kernel::dsv3_router_gemm",
    "op_mapping": "matmul",
    "operation": "out = mat1 @ mat2",
    "source_code": "sgl_kernel/gemm.py(262): dsv3_router_gemm",
    "call_stack": "cudaLaunchKernelExC <- sgl_kernel::dsv3_router_gemm <- <built-in method dsv3_router_gemm of PyCapsule object at 0x7f9751cc2130> <- torch/_ops.py(1147): __call__ <- sgl_kernel/gemm.py(262): dsv3_router_gemm <- sglang/srt/models/deepseek_v2.py(243): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: MoEGate_0 <- sglang/srt/models/deepseek_v2.py(492): forward_normal <- sglang/srt/models/deepseek_v2.py(446): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2MoE_0 <- sglang/srt/models/deepseek_v2.py(1880): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2DecoderLayer_1 <- sglang/srt/models/deepseek_v2.py(2030): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(2159): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1537): forward_extend <- sglang/srt/model_executor/model_runner.py(1621): _forward_raw <- sglang/srt/model_executor/model_runner.py(1594): forward <- sglang/srt/managers/tp_worker.py(215): forward_batch_generation <- sglang/srt/managers/tp_worker_overlap_thread.py(152): forward_thread_func_ <- torch/utils/_contextlib.py(116): decorate_context <- sglang/srt/managers/tp_worker_overlap_thread.py(140): forward_thread_func <- threading.py(953): run <- threading.py(1016): _bootstrap_inner <- threading.py(973): _bootstrap",
    "params": [
      {
        "id": 0,
        "role": "output",
        "example_dim": [
          1,
          256
        ],
        "example_dtype": "c10::BFloat16",
        "description": "out"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat1"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          256,
          7168
        ],
        "example_dtype": "c10::BFloat16",
        "description": "mat2"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<int, int, int, at::native::minimum_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#3}::operator()() const::{lambda(int, int)#1}>, std::array<char*, 3ul> >(int, at::native::BinaryFunctor<int, int, int, at::native::minimum_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#3}::operator()() const::{lambda(int, int)#1}>, std::array<char*, 3ul>)",
    "kernel_implementation": "aten::minimum",
    "op_mapping": "TBD",
    "operation": "out[i] = input[i] if input[i] < other[i] else other[i]",
    "source_code": "",
    "call_stack": "cudaLaunchKernel <- aten::minimum <- aten::min <- <built-in method min of type object at 0x7f1f468f6f40> <- sglang/srt/model_executor/forward_batch_info.py(762): get_prefix_chunk_seq_lens <- sglang/srt/model_executor/forward_batch_info.py(785): prepare_chunked_prefix_cache_info <- sglang/srt/models/deepseek_v2.py(1738): forward_normal_chunked_kv_core <- sglang/srt/models/deepseek_v2.py(1134): forward_core <- sglang/srt/models/deepseek_v2.py(1077): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1880): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(2030): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(2159): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1537): forward_extend <- sglang/srt/model_executor/model_runner.py(1621): _forward_raw <- sglang/srt/model_executor/model_runner.py(1594): forward <- sglang/srt/managers/tp_worker.py(215): forward_batch_generation <- sglang/srt/managers/tp_worker_overlap_thread.py(152): forward_thread_func_ <- torch/utils/_contextlib.py(116): decorate_context <- sglang/srt/managers/tp_worker_overlap_thread.py(140): forward_thread_func <- threading.py(953): run <- threading.py(1016): _bootstrap_inner <- threading.py(973): _bootstrap",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          7
        ],
        "example_dtype": "int",
        "description": "input"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          1,
          7
        ],
        "example_dtype": "int",
        "description": "other"
      }
    ]
  },
  {
    "kernel_name": "void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#3}::operator()() const::{lambda(int)#1}, std::array<char*, 2ul> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#3}::operator()() const::{lambda(int)#1}, std::array<char*, 2ul>)",
    "kernel_implementation": "aten::clamp",
    "op_mapping": "TBD",
    "operation": "out=max(min(input,max_clamp),min_clamp)",
    "source_code": "",
    "call_stack": "cudaLaunchKernel <- aten::clamp <- <built-in method clamp of Tensor object at 0x7f1a6e2c2c50> <- sglang/srt/model_executor/forward_batch_info.py(762): get_prefix_chunk_seq_lens <- sglang/srt/model_executor/forward_batch_info.py(785): prepare_chunked_prefix_cache_info <- sglang/srt/models/deepseek_v2.py(1738): forward_normal_chunked_kv_core <- sglang/srt/models/deepseek_v2.py(1134): forward_core <- sglang/srt/models/deepseek_v2.py(1077): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1880): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(2030): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(2159): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1537): forward_extend <- sglang/srt/model_executor/model_runner.py(1621): _forward_raw <- sglang/srt/model_executor/model_runner.py(1594): forward <- sglang/srt/managers/tp_worker.py(215): forward_batch_generation <- sglang/srt/managers/tp_worker_overlap_thread.py(152): forward_thread_func_ <- torch/utils/_contextlib.py(116): decorate_context <- sglang/srt/managers/tp_worker_overlap_thread.py(140): forward_thread_func <- threading.py(953): run <- threading.py(1016): _bootstrap_inner <- threading.py(973): _bootstrap",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          1,
          7
        ],
        "example_dtype": "int",
        "description": "input"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [],
        "example_dtype": "Scalar",
        "description": "min_clamp"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [],
        "example_dtype": "",
        "description": "max_clamp"
      }
    ]
  },
  {
    "kernel_name": "create_chunked_prefix_cache_kv_indices",
    "kernel_implementation": "<built-in function launch>",
    "op_mapping": "TBD",
    "operation": "TBD",
    "source_code": "",
    "call_stack": "cuLaunchKernel <- <built-in function launch> <- triton/backends/nvidia/driver.py(522): __call__ <- triton/runtime/jit.py(525): run <- triton/runtime/jit.py(347): <lambda> <- sglang/srt/model_executor/forward_batch_info.py(580): prepare_chunked_kv_indices <- sglang/srt/model_executor/forward_batch_info.py(785): prepare_chunked_prefix_cache_info <- sglang/srt/models/deepseek_v2.py(1738): forward_normal_chunked_kv_core <- sglang/srt/models/deepseek_v2.py(1134): forward_core <- sglang/srt/models/deepseek_v2.py(1077): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1880): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(2030): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(2159): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1537): forward_extend <- sglang/srt/model_executor/model_runner.py(1621): _forward_raw <- sglang/srt/model_executor/model_runner.py(1594): forward <- sglang/srt/managers/tp_worker.py(215): forward_batch_generation <- sglang/srt/managers/tp_worker_overlap_thread.py(152): forward_thread_func_ <- torch/utils/_contextlib.py(116): decorate_context <- sglang/srt/managers/tp_worker_overlap_thread.py(140): forward_thread_func <- threading.py(953): run <- threading.py(1016): _bootstrap_inner <- threading.py(973): _bootstrap",
    "params": [
      {
        "id": 0,
        "role": "unknown",
        "example_dim": "N/A",
        "example_dtype": "N/A",
        "description": ""
      }
    ]
  },
  {
    "kernel_name": "void merge_attn_states_kernel<__nv_bfloat16, 128u>(__nv_bfloat16*, float*, __nv_bfloat16 const*, float const*, __nv_bfloat16 const*, float const*, unsigned int, unsigned int, unsigned int)",
    "kernel_implementation": "sgl_kernel::merge_state_v2",
    "op_mapping": "TBD",
    "operation": "TBD",
    "source_code": "sgl_kernel/attention.py(31): merge_state_v2",
    "call_stack": "cudaLaunchKernel <- sgl_kernel::merge_state_v2 <- <built-in method  of PyCapsule object at 0x7f1a6e2cf0c0> <- torch/_ops.py(755): __call__ <- sgl_kernel/attention.py(31): merge_state_v2 <- sglang/srt/models/deepseek_v2.py(1635): _chunked_prefix_attn_mha <- sglang/srt/models/deepseek_v2.py(1738): forward_normal_chunked_kv_core <- sglang/srt/models/deepseek_v2.py(1134): forward_core <- sglang/srt/models/deepseek_v2.py(1077): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2AttentionMLA_0 <- sglang/srt/models/deepseek_v2.py(1880): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2DecoderLayer_0 <- sglang/srt/models/deepseek_v2.py(2030): forward <- torch/nn/modules/module.py(1755): _call_impl <- nn.Module: DeepseekV2Model_0 <- sglang/srt/models/deepseek_v2.py(2159): forward <- torch/utils/_contextlib.py(113): decorate_context <- sglang/srt/model_executor/model_runner.py(1537): forward_extend <- sglang/srt/model_executor/model_runner.py(1621): _forward_raw <- sglang/srt/model_executor/model_runner.py(1594): forward <- sglang/srt/managers/tp_worker.py(215): forward_batch_generation <- sglang/srt/managers/tp_worker_overlap_thread.py(152): forward_thread_func_ <- torch/utils/_contextlib.py(116): decorate_context <- sglang/srt/managers/tp_worker_overlap_thread.py(140): forward_thread_func <- threading.py(953): run <- threading.py(1016): _bootstrap_inner <- threading.py(973): _bootstrap",
    "params": [
      {
        "id": 0,
        "role": "input",
        "example_dim": [
          98305,
          16,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "v_a"
      },
      {
        "id": 1,
        "role": "input",
        "example_dim": [
          98305,
          16
        ],
        "example_dtype": "float",
        "description": "s_a"
      },
      {
        "id": 2,
        "role": "input",
        "example_dim": [
          98305,
          16,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "v_b"
      },
      {
        "id": 3,
        "role": "input",
        "example_dim": [
          98305,
          16
        ],
        "example_dtype": "float",
        "description": "s_b"
      },
      {
        "id": 4,
        "role": "input",
        "example_dim": [
          98305,
          16,
          128
        ],
        "example_dtype": "c10::BFloat16",
        "description": "v_merged"
      },
      {
        "id": 5,
        "role": "input",
        "example_dim": [
          98305,
          16
        ],
        "example_dtype": "float",
        "description": "s_merged"
      }
    ]
  }
]
