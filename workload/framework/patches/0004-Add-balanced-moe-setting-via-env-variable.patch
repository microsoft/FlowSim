From 68e998d671a0209b560be209de01fa5e89d3df8d Mon Sep 17 00:00:00 2001
From: Tao Zhang <zhangt@microsoft.com>
Date: Wed, 27 Aug 2025 10:00:52 -0700
Subject: [PATCH 4/5] Add balanced moe setting via env variable

---
 .../layers/moe/fused_moe_triton/fused_moe.py   | 11 ++++++++++-
 python/sglang/srt/models/deepseek_v2.py        | 18 +++++++++++++++++-
 2 files changed, 27 insertions(+), 2 deletions(-)

diff --git a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
index d2c65d973..bc590c564 100644
--- a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
+++ b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
@@ -61,7 +61,7 @@ if _is_cuda or _is_hip:
 
 logger = logging.getLogger(__name__)
 padding_size = 128 if bool(int(os.getenv("SGLANG_MOE_PADDING", "0"))) else 0
-
+balanced_moe = bool(int(os.getenv("SGLANG_BALANCED_MOE", "0")))
 
 @triton.jit
 def write_zeros_to_output(
@@ -1441,6 +1441,15 @@ def fused_experts_impl(
 
         curr_topk_ids = topk_ids[begin_chunk_idx:end_chunk_idx]
         curr_topk_weights = topk_weights[begin_chunk_idx:end_chunk_idx]
+        # Patch topk ids equally to experts
+        if balanced_moe:
+            logger.warning("Using balanced MoE with uniform topk ids. This is for benchmarking only!")
+            if curr_topk_ids.numel() > 0:
+                num_tokens, k = curr_topk_ids.shape
+                expert_ids_uniform = torch.arange(E, device=curr_topk_ids.device).repeat((num_tokens * k + E - 1) // E)[:num_tokens * k]
+                expert_ids_uniform = expert_ids_uniform.view(num_tokens, k)
+                curr_topk_ids = expert_ids_uniform
+        # --- Patch end ---
 
         sorted_token_ids, expert_ids, num_tokens_post_padded = moe_align_block_size(
             curr_topk_ids, config["BLOCK_SIZE_M"], E
diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py
index b5b13d9ac..ac36d2d2e 100644
--- a/python/sglang/srt/models/deepseek_v2.py
+++ b/python/sglang/srt/models/deepseek_v2.py
@@ -150,7 +150,7 @@ _is_sm100_supported = is_cuda() and is_sm100_supported()
 
 
 logger = logging.getLogger(__name__)
-
+balanced_moe = bool(int(os.getenv("SGLANG_BALANCED_MOE", "0")))
 
 class AttnForwardMethod(IntEnum):
     # Use multi-head attention
@@ -603,6 +603,22 @@ class DeepseekV2MoE(nn.Module):
                 (0, self.top_k), dtype=torch.float32, device=hidden_states.device
             )
 
+        ## ----EP Patch Start: balance the expert distribution----
+        if balanced_moe:
+            logger.warning("Using balanced MoE with uniform topk ids. This is for benchmarking only!")
+            # Uniformly assign different experts to each token's top-k selection
+            if topk_idx.numel() > 0:
+                num_tokens, k = topk_idx.shape
+                E = self.num_experts
+                expert_ids_uniform = torch.arange(
+                    E, device=topk_idx.device
+                ).repeat((num_tokens * k + E - 1) // E)[:num_tokens * k]
+                expert_ids_uniform = expert_ids_uniform.view(num_tokens, k)
+                topk_idx = expert_ids_uniform
+                topk_weights = torch.full_like(
+                    topk_weights, 1.0 / k
+                )  # Uniform weights
+        ## ----EP Patch End: balance the expert distribution----
         final_hidden_states = self.experts(
             hidden_states=hidden_states,
             topk_idx=topk_idx,
-- 
2.43.0

